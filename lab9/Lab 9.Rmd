---
title: "Data119 - Lab 9"
output: 
   learnr::tutorial:
      css: css/custom-styles.css
runtime: shiny_prerendered
header-includes:
  -"\usepackage{tcolorbox}"
  -"\newtcolorbox{discussionbox}{
  colback=#ddd,
  colframe=#800000,
  coltext=black,
  boxsep=5pt,
  arc=4pt}"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)

library(shiny)
library(ggplot2)
library(dplyr)
library(tidyr)
#library(DBI)
library(gradethis)
library(learnr)
library(reticulate)
#library(RSQLite)


custom_checker <- function(label, user_code, solution_code, envir_result, evaluate_result, last_value, stage, ...) {
  if (stage == "code_check") {
      if (user_code == solution_code){
          return(list(message = random_praise(), correct = TRUE))
      }
    return(list(message = random_encouragement(), correct = FALSE))
  }
}

gradethis_setup()
tutorial_options(exercise.completion = FALSE, exercise.checker = custom_checker)
```

```{r header, echo = FALSE}
library(htmltools)

tags$div(
  class = "topContainer",
  tags$div(
    class = "logoAndTitle",
    tags$img(
      src = "./images/dsi_logo.png",
      alt = "DSI Logo",
      class = "topLogo"
    ),
    tags$h1("Data 119 Review", class = "pageTitle")
  )
)
```

## Goals

The goal of this lab is to help you review topics we've covered this quarter.

## Concept Review

In the next few sections, you will review what we learned in class by using a few interactive widgets. Investigate the best choices of models, variables, transformations, and hyperparameters and then answer the questions that follow.

### Reviewing Regression

Use the widget below to review what we learned about multiple linear regression:

<!--```{r regression_app, echo = FALSE}
library(glmnet)

# Generate synthetic data
set.seed(123)
n <- 200
x1 <- rnorm(n, 2, 2)
x2 <- rexp(n)
x3 <- rnorm(n)
x4 <- rbinom(n,7,.3)
x5 <- rnorm(n,10,2)
y <- (3 + 2*x1 - 1.5*x2 + 0.5*x3 + x5)^2 + x3^4 + 6*x3*x5 + x5^3 + rnorm(n)

data <- data.frame(x1, x2, x3, x4, x5, y)

# UI
ui <- fluidPage(
  titlePanel("Regularized Regression with Transformations & Polynomial Features"),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("reg_method", "Regularization Type:", choices = c("Ridge", "Lasso")),
      checkboxGroupInput("features", "Select Features:", 
                         choices = names(data)[-6], 
                         selected = names(data)[-6]),
      
      selectInput("transform", "Transform Response Variable (y):", choices = c("None", "Log", "Sqrt"), selected = "None"),
      sliderInput("poly_degree", "Degree of Polynomial Features:", min = 1, max = 5, value = 1),
      
      sliderInput("alpha", "Regularization Parameter (α):", min = 0, max = 100, value = 0, step = 0.01)
    ),
    
    mainPanel(
      h3("Model Performance"),
      tableOutput("metrics"),
      
      h3("Coefficient Paths"),
      plotOutput("coef_plot"),
      
      h3("Residual Plot"),
      plotOutput("residual_plot"),
      
      h3("Residual Distribution"),
      plotOutput("residual_dist")
    )
  )
)

# Server
server <- function(input, output) {
  
  model_data <- reactive({
    req(input$features)  # Ensure at least one feature is selected
    df <- data %>% select(all_of(input$features), y)
    
    # Ensure y is numeric
    df$y <- as.numeric(df$y)
    
    # Apply transformation safely
    if (input$transform == "Log") {
      if (min(df$y) <= 0) df$y <- df$y - min(df$y) + 1  # Shift to positive
      df$y <- log(df$y)
    } else if (input$transform == "Sqrt") {
      if (min(df$y) < 0) df$y <- df$y - min(df$y) + 1  # Shift to non-negative
      df$y <- sqrt(df$y)
    }
    
    # Apply polynomial features properly
    if (input$poly_degree > 1) {
      poly_data <- df %>% select(-y)  # Select only predictor columns
      poly_expanded <- lapply(poly_data, function(col) {
        as.data.frame(poly(col, degree = input$poly_degree, raw = TRUE))  # Convert each column separately
      })
      
      poly_df <- bind_cols(poly_expanded)  # Merge polynomial features
      colnames(poly_df) <- paste0(rep(names(poly_data), each = input$poly_degree), "_poly", 1:input$poly_degree)
      
      df <- cbind(poly_df, y = df$y)  # Merge back with y
    }
    
    return(df)
  })
  
  fit_model <- reactive({
    req(length(input$features) > 0)  # Ensure at least one feature is selected
    
    df <- model_data()
    x <- as.matrix(df %>% select(-y))
    y <- df$y
    
    lambda_value <- input$alpha  # Alpha is now the regularization penalty
    alpha_value <- ifelse(input$reg_method == "Ridge", 0, 1) # Ridge = 0, Lasso = 1
    model <- glmnet(x, y, alpha = alpha_value, lambda = lambda_value)
    
    return(list(model = model, x = x, y = y))
  })
  
  output$metrics <- renderTable({
    res <- fit_model()
    model <- res$model
    lambda_value <- input$alpha
    
    preds <- predict(model, newx = res$x, s = lambda_value)
    mse <- mean((res$y - preds)^2)
    r_squared <- 1 - (mse / var(res$y))
    
    data.frame(
      Metric = c("MSE", "R-squared"),
      Value = c(round(mse, 4), round(r_squared, 4))
    )
  })
  
  output$coef_plot <- renderPlot({
    req(input$reg_method %in% c("Ridge", "Lasso"))  # Ensure Ridge or Lasso is selected
    
    res <- fit_model()
    alpha_value <- ifelse(input$reg_method == "Ridge", 0, 1) # Ridge = 0, Lasso = 1
    
    # Fit model across different values of lambda
    lambda_seq <- exp(seq(log(0.001), log(100), length.out = 50))  # Log-spaced λ values
    
    coef_paths <- lapply(lambda_seq, function(l) {
      model <- glmnet(res$x, res$y, alpha = alpha_value, lambda = l)
      coefs <- as.matrix(coef(model))[-1, , drop = FALSE]  # Ensure correct dimension
      data.frame(Feature = rownames(coefs), Coefficient = as.vector(coefs), Lambda = l)
    })
    
    coef_df <- do.call(rbind, coef_paths)  # Merge into one dataframe
    
    best_lambda <- input$alpha  # Now alpha is used as the regularization penalty
    
    ggplot(coef_df, aes(x = Lambda, y = Coefficient, color = Feature)) +
      geom_line() +
      geom_vline(xintercept = best_lambda, linetype = "dashed", color = "red") +
      scale_x_log10() +
      labs(title = paste(input$reg_method, "Coefficient Paths Across Lambda Values"),
           x = "Lambda (Regularization Penalty)", y = "Coefficient") +
      theme_minimal()
  })
  
  output$residual_plot <- renderPlot({
    res <- fit_model()
    model <- res$model
    preds <- as.vector(predict(model, newx = res$x, s = input$alpha))  # Ensure correct dimension
    residuals <- res$y - preds
    
    ggplot(data.frame(Predicted = preds, Residuals = residuals), aes(x = Predicted, y = Residuals)) +
      geom_point(alpha = 0.7, color = "blue") +
      geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
      theme_minimal() +
      labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals")
  })
  
  output$residual_dist <- renderPlot({
    res <- fit_model()
    model <- res$model
    preds <- as.vector(predict(model, newx = res$x, s = input$alpha))  # Ensure correct dimension
    residuals <- res$y - preds
    
    ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
      geom_histogram(aes(y = ..density..), bins = 20, fill = "blue", alpha = 0.6) +
      geom_density(color = "red", linewidth = 1) +
      theme_minimal() +
      labs(title = "Residual Distribution", x = "Residuals", y = "Density")
  })
  
}

# Run the app
shinyApp(ui = ui, server = server)
```
-->

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

1. Which transformation, if any, would you choose for this dataset?
2. What degree of polynomial features would you choose?
3. Does regularization improve performance?
4. Which produces better results LASSO or Ridge Regression?
5. What value of the regularization penalty produces the best results for LASSO? For Ridge Regression?
6. Overall, what combination of choices gets you the best performance? Write the metrics for your best model on the board next to your group number. Can you outperform other groups?
::::

### Reviewing Classification

Use the widget below to review what we learned about classification:

```{r classification_app, echo=FALSE}
library(shiny)
library(ggplot2)
library(caret)
library(pROC)
library(dplyr)

# Generate synthetic data
set.seed(123)
n <- 200
x1 <- rnorm(n)
x2 <- rnorm(n)
prob <- 1 / (1 + exp(- (0.5 * x1 + 0.8 * x2))) # Logistic function
y <- rbinom(n, 1, prob) # Generate binary labels

data <- data.frame(x1, x2, y)

# Fit logistic regression model
model <- glm(y ~ x1 + x2, data = data, family = "binomial")
data$predicted_prob <- predict(model, type = "response")

# UI
ui <- fluidPage(
  titlePanel("Threshold Effect on Classification Metrics"),
  
  sidebarLayout(
    sidebarPanel(
      sliderInput("threshold", "Threshold:", min = 0, max = 1, value = 0.5, step = 0.01),
      h3("ROC Curve"),
      plotOutput("roc_curve", height = "300px") # ROC curve directly below the slider
    ),
    
    mainPanel(
      h3("Confusion Matrix"),
      htmlOutput("conf_matrix"), # Confusion matrix with bold labels
      
      h3("Model Evaluation Metrics"),
      tableOutput("metrics"),
      
      h3("Data (First 10 Rows)"),
      tableOutput("data_snippet")  # New: Data table snippet
    )
  )
)

# Server
server <- function(input, output) {
  
  pred_class <- reactive({
    ifelse(data$predicted_prob >= input$threshold, 1, 0)
  })
  
  confusion_stats <- reactive({
    cm <- confusionMatrix(factor(pred_class()), factor(data$y))
    as.matrix(cm$table)
  })
  
  output$conf_matrix <- renderUI({
    cm <- confusion_stats()
    HTML(sprintf("
      <table class='table table-bordered' style='text-align:center; font-size:16px;'>
        <tr>
          <th></th>
          <th>Predicted 0</th>
          <th>Predicted 1</th>
        </tr>
        <tr>
          <td><b>Actual 0</b></td>
          <td>%d</td>
          <td>%d</td>
        </tr>
        <tr>
          <td><b>Actual 1</b></td>
          <td>%d</td>
          <td>%d</td>
        </tr>
      </table>
    ", cm[1,1], cm[1,2], cm[2,1], cm[2,2]))
  })
  
  output$metrics <- renderTable({
    cm <- confusion_stats()
    TN <- cm[1, 1]
    FP <- cm[1, 2]
    FN <- cm[2, 1]
    TP <- cm[2, 2]
    
    sensitivity <- TP / (TP + FN)
    specificity <- TN / (TN + FP)
    accuracy <- (TP + TN) / (TP + TN + FP + FN)
    
    data.frame(
      Metric = c("Sensitivity (Recall)", "Specificity", "Accuracy"),
      Equation = c("TP / (TP + FN)", "TN / (TN + FP)", "(TP + TN) / (TP + TN + FP + FN)"),
      Computation = c(
        sprintf("%d / (%d + %d)", TP, TP, FN),
        sprintf("%d / (%d + %d)", TN, TN, FP),
        sprintf("(%d + %d) / (%d + %d + %d + %d)", TP, TN, TP, TN, FP, FN)
      ),
      Value = c(
        sprintf("%.4f", sensitivity),
        sprintf("%.4f", specificity),
        sprintf("%.4f", accuracy)
      )
    )
  }, align = "lccc", digits = 4)
  
  output$data_snippet <- renderTable({
    snippet <- data %>%
      mutate(Prediction = ifelse(predicted_prob >= input$threshold, 1, 0)) %>%
      select(x1, x2, predicted_prob, Prediction) %>%
      head(10)
    
    colnames(snippet) <- c("X1", "X2", "Predicted Probability", "Prediction")
    snippet
  }, digits = 4)
  
  output$roc_curve <- renderPlot({
    roc_obj <- roc(data$y, data$predicted_prob)
    ggplot(data.frame(tpr = roc_obj$sensitivities, fpr = 1 - roc_obj$specificities), aes(x = fpr, y = tpr)) +
      geom_line() +
      geom_abline(linetype = "dashed") +
      geom_point(aes(x = 1 - roc_obj$specificities[which.min(abs(roc_obj$thresholds - input$threshold))], 
                     y = roc_obj$sensitivities[which.min(abs(roc_obj$thresholds - input$threshold))]), 
                 color = "red", size = 3) +
      labs(x = "False Positive Rate", y = "True Positive Rate", title = "ROC Curve") +
      theme_minimal()
  })
}

# Run the app
shinyApp(ui = ui, server = server)
```

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

1. As the threshold changes, how does this affect model performance?
2. What threshold gets you the highest accuracy? sensitivity? specificity?
3. Why did we use Logistic Regression instead of KNN to illustrate this?
4. Are there changes we could make to our KNN algorithm that would allow us to get a probability so that we can choose a threshold?
::::

### Reviewing Clustering

Use the widget below to review what we learned about clustering:

```{r clustering_app, echo=FALSE}
library(shiny)
library(ggplot2)
library(cluster)
library(dendextend)

# Generate synthetic data
set.seed(123)
n <- 200
x <- rnorm(n)
y <- rnorm(n)
data <- data.frame(x, y)

# UI
ui <- fluidPage(
  titlePanel("Clustering Exploration"),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("clust_method", "Clustering Method:", choices = c("K-Means", "Hierarchical")),
      
      # Options for Hierarchical Clustering
      conditionalPanel(
        condition = "input.clust_method == 'Hierarchical'",
        selectInput("dist_metric_hc", "Distance Metric:", choices = c("euclidean", "manhattan")),
        selectInput("linkage", "Linkage Method:", choices = c("complete", "single", "average", "centroid")),
        sliderInput("cut_height", "Dendrogram Cut Height:", min = 0.5, max = 5, value = 2, step = 0.1)
      ),
      
      # Options for K-Means Clustering
      conditionalPanel(
        condition = "input.clust_method == 'K-Means'",
        selectInput("dist_metric_km", "Distance Metric:", choices = c("euclidean", "manhattan")),
        sliderInput("k_value", "Number of Clusters (k):", min = 2, max = 10, value = 3, step = 1)
      )
    ),
    
    mainPanel(
      conditionalPanel(
        condition = "input.clust_method == 'Hierarchical'",
        plotOutput("dendrogram"),
        plotOutput("hier_scatter")
      ),
      conditionalPanel(
        condition = "input.clust_method == 'K-Means'",
        plotOutput("elbow_plot"),
        plotOutput("kmeans_scatter")
      )
    )
  )
)

# Server
server <- function(input, output) {
  
  # Compute hierarchical clustering distance matrix
  dist_matrix_hc <- reactive({
    dist(data, method = input$dist_metric_hc)
  })
  
  # Compute hierarchical clustering
  hc <- reactive({
    hclust(dist_matrix_hc(), method = input$linkage)
  })
  
  # Render dendrogram with smaller labels
  output$dendrogram <- renderPlot({
    dend <- as.dendrogram(hc())
    dend <- color_branches(dend, h = input$cut_height)
    dend <- set(dend, "labels_cex", 0.5)  # This makes labels smaller
    plot(dend, main = "Hierarchical Clustering Dendrogram",)
    abline(h = input$cut_height, col = "red", lty = 2)
  })
  
  # Render scatterplot for hierarchical clustering
  output$hier_scatter <- renderPlot({
    clusters <- cutree(hc(), h = input$cut_height)
    ggplot(data, aes(x, y, color = factor(clusters))) +
      geom_point(size = 3) +
      labs(title = "Hierarchical Clustering", color = "Cluster") +
      theme_minimal()
  })
  
  # Compute distance matrix for K-means clustering
  kmeans_data <- reactive({
    if (input$dist_metric_km == "euclidean") {
      return(data)
    } else {
      return(as.matrix(dist(data, method = "manhattan")))  # Convert Manhattan to a format K-means can use
    }
  })
  
  # Compute K-means clustering
  kmeans_result <- reactive({
    kmeans(kmeans_data(), centers = input$k_value, nstart = 10)
  })
  
  # Render K-means scatterplot
  output$kmeans_scatter <- renderPlot({
    ggplot(data, aes(x, y, color = factor(kmeans_result()$cluster))) +
      geom_point(size = 3) +
      labs(title = "K-Means Clustering", color = "Cluster") +
      theme_minimal()
  })
  
  # Compute WCSS (Within-cluster sum of squares) for Elbow plot
  wcss <- reactive({
    sapply(1:10, function(k) kmeans(kmeans_data(), centers = k, nstart = 10)$tot.withinss)
  })
  
  # Render elbow plot
  output$elbow_plot <- renderPlot({
    elbow_df <- data.frame(k = 1:10, wcss = wcss())
    ggplot(elbow_df, aes(x = k, y = wcss)) +
      geom_line() +
      geom_point() +
      geom_point(aes(x = input$k_value, y = wcss()[input$k_value]), color = "red", size = 4) +
      labs(title = "Elbow Method for K-Means", x = "Number of Clusters (k)", y = "WCSS") +
      theme_minimal()
  })
}

# Run the app
shinyApp(ui = ui, server = server)

```

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

1. What value of K would you choose for K-Means clustering? Why? Does your answer change depending on the distance metric you choose?
2. What happens when you use single linkage for hierarchical clustering? Does changing the distance/dissimilarity metric help?
3. What happens when you use centroid linkage for hierarchical clustering? Does changing the distance/dissimilarity metric help?
4. Which linkage and dissimilary pair seems to produce the most balanced dendrogram?
5. Where would you cut this dendrogram to create the same number of clusters you chose for K-Means?
6. Based on the dendrogram, would you choose the same number of clusters as you used for K-Means? Why or why not?
::::

## Implementation Review

Now that we've reminded ourselves of the concepts we learned this quarter, let's implement some of them in Python.

### Setup

For this lab we will be using `plotnine`, `pandas`, `numpy`, `scikit-learn`, `scipy`, and the dataset `msleep`.

```{python setup-packages, exercise=TRUE, exercise.eval = FALSE, message = FALSE}
import numpy as np
import pandas as pd
import plotnine as p9
import scipy
import sklearn

# To download this file go to https://posit.ds.uchicago.edu/data119-lab9/www/msleep.csv

msleep = pd.read_csv("./data/msleep.csv")
msleep.head()
```

This dataset has 83 rows and 11 variables and contains information about sleep times for various mammals taken from V. M. Savage and G. B. West. *A quantitative, theoretical framework for understanding mammalian sleep.* Proceedings of the National Academy of Sciences, 104 (3):1051-1056, 2007. The variables and their descriptions are listed below:

- `name`: common name
- `genus`
- `vore`: carnivore, omnivore or herbivore?
- `order`
- `conservation`: the conservation status of the animal
- `sleep_total`: total amount of sleep, in hours
- `sleep_rem`: rem sleep, in hours
- `sleep_cycle`: length of sleep cycle, in hours
- `awake`: amount of time spent awake, in hours
- `brainwt`: brain weight in kilograms
- `bodywt`: body weight in kilograms

You can use the following cell to explore the data>

```{python explore, exercise=TRUE, exercise.eval = FALSE, message = FALSE}

```

### Modeling `sleep_total`

We want to learn if there are certain variables that we can use to help predict how long a mammal will sleep.

```{r q1, echo=FALSE}
question(
  "What type of model should I use?",
  answer("Supervised", correct = TRUE, message = "That's right! Since we have values for a response variable that we can use to build the model and calculate a loss/error, we would use supervised learning."),
  answer("Unsupervised", message = "Not quite. Unsupervised learning does not involve a response variable."),
  allow_retry = TRUE,
  post_message = "There are two main types of supervised Learning: regression and classification. Do you remember when to use each?"
)
```

```{r q2, echo=FALSE}
question(
  "More specifically, which model should I use?",
  answer("KNN",  message = "Not quite. KNN (K Nearest Neighbors) is a classification algorithm which does not allow us to model continuous response variables."),
  answer("Simple linear regression", message = "Very close! Simple linear regression allows us to model a continuous response variable. But, it only allows us to use one predictor and I want to investigate more than one..."),
  answer("Multiple linear regression", correct = TRUE, message = "Correct! Multiple linear regresison will allow us to model a continuous response variable using multiple features."),
  answer("Logistic regression", message = "Not quite. Even though regression is in the name, logistic regression is a classification algorithm which does not allow us to model continuous response variables."),
  answer("K-Means", message = "Not quite. K-Means is a method of clustering which is an unsupervised learning algorithm."),
  allow_retry = TRUE
)
```

Now, that we've narrowed down the type of model we want to build, we need to check that our data meets the model assumptions. Remember that MLR assumes the response variable is normally distributed. Use the next cell to check this assumption:

```{python sleep_hist, exercise=TRUE, exercise.eval = TRUE, message = FALSE}
(p9.ggplot(msleep, p9.aes(x = ___)) +
  p9.geom_histogram(bins = 24))
```

```{python sleep_hist-solution, message = FALSE, warning = FALSE, echo = FALSE}
(p9.ggplot(msleep, p9.aes(x = sleep_total)) +
  p9.geom_histogram(bins = 24))
```

```{r sleep_hist-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q3, echo=FALSE}
question(
  "Does the data meet this assumption?",
  answer("Yes!",  message = ""),
  answer("No...", message = ""),
  allow_retry = TRUE
)
```

Now, we need to choose which variables to include in our model. Before you do any additional analysis, are there any variables you would be concerned about including in the model based on the descriptions?  

```{r q4, echo=FALSE}
question_checkbox(
  "Select the variables that you are concerned about including in the model. ",
  answer("name"),
  answer("genus"),
  answer("vore"),
  answer("order"),
  answer("conservation"),
  answer("sleep_rem"),
  answer("sleep_cycle"),
  answer("awake"),
  answer("brain_wt"),
  answer("bodywt"),
  answer_fn(function(value) {
    correct("There are multiple correct answers to this question, but the variables I found concerning were name (unique for every animal) and sleep_rem, sleep_cycle, and awake (these should be highly related with sleep_total but might not help us learn much about what contributes to how much an animal sleeps). bodywt and brainwt might also be very similar to each other which could be an issue...")
  }),
  allow_retry = TRUE
)
```

Generate a correlation matrix for each potential numeric predictor variable.

```{python cormat, exercise=TRUE, exercise.eval = FALSE, message = FALSE}
round(msleep[[ ___ ]].corr(), 3)
```

```{python cormat-solution, message = FALSE, warning = FALSE, echo = FALSE}
round(msleep[['sleep_rem','sleep_cycle','awake','brain_wt','bodywt']].corr(), 3)
```

```{r cormat-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

Now, let's one-hot-encode our categorical variables. Don't forget you need a reference level!

```{python dummy_drop, exercise = TRUE, message = FALSE}
msleep = pd.get_dummies(___, columns = ___, dtype = float, ___)
```

```{python dummy_drop-solution, message = FALSE, warning = FALSE, echo = FALSE}
msleep = pd.get_dummies(msleep, columns = ['genus', 'vore', 'order', 'conservation'], dtype = float, drop_first=True)
```

```{r dummy_drop-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{python setup1, message = FALSE, warning = FALSE, echo = FALSE}
msleep = pd.get_dummies(msleep, columns = ['genus', 'vore', 'order', 'conservation'], dtype = float, drop_first=True)
```

We still need to do one more thing to help us decide which variables to use - calculate the VIF. Calculate the VIF for all potential predictor variables (don't include `name` or the response variable).

```{python vif, exercise = TRUE, exercise.eval = FALSE, message = FALSE, warning = FALSE, exercise.setup = "setup1"}
VIF_msleep = [variance_inflation_factor(msleep.drop(___).values, i) for i in range(len(___.columns))]
VIF_msleep_df = pd.DataFrame({"feature": ___.columns, "VIF": ___})
VIF_msleep_df
```

```{python vif-solution, message = FALSE, warning = FALSE, echo = FALSE}
VIF_msleep = [variance_inflation_factor(msleep.drop(columns=['name','sleep_total']).values, i) for i in range(len(msleep.drop(columns=['name','sleep_total']).columns))]
VIF_msleep_df = pd.DataFrame({"feature": msleep.drop(columns=['name','sleep_total']).columns, "VIF": VIF_msleep})
VIF_msleep_df
```

```{r vif-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q5, echo=FALSE}
question_checkbox(
  "After looking at your correlation matrix and VIFs, select the variables that you are concerned about including in the model. ",
  answer("genus"),
  answer("vore"),
  answer("order"),
  answer("conservation"),
  answer("sleep_rem"),
  answer("sleep_cycle"),
  answer("awake"),
  answer("brain_wt"),
  answer("bodywt"),
  answer_fn(function(value) {
    correct("There are multiple correct answers to this question, but the variables I found concerning were ")
  }),
  allow_retry = TRUE
)
```

Let's prepare our data to fit a regression model. Split the data into `X` and `y` where `X` contains the variables ABCDS and `y` contains the response variable. Then, use `train_test_split()` to create a training set with 70% of the data and a testing set with the remaining 30% of the data. Lastly, fit a `StandardScaler()` on the training set and us it to scale both the training and the testing sets. Make sure that you are only scaling the numeric variables.

```{python pp, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup1"}
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = ___
y = ___

X_train, X_test, y_train, y_test = train_test_split(___, random_state = 314)
                                                       
scaler = StandardScaler()

X_train_pp = X_train.copy()
X_test_pp = X_test.copy()

col_names = [___]
scaler.fit(___)
X_train_pp[col_names] = scaler.transform(___)

X_test_pp[col_names] = scaler.transform(___)
```

```{python pp-solution, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup1"}
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = msleep[[]]
y = msleep['sleep_total']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 314)
                                                       
scaler = StandardScaler()

X_train_pp = X_train.copy()
X_test_pp = X_test.copy()

col_names = []
scaler.fit(X_train_pp[col_names])
X_train_pp[col_names] = scaler.transform(X_train_pp[col_names])

X_test_pp[col_names] = scaler.transform(X_test_pp[col_names])
```

```{r solution-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{python setup2, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup1"}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = msleep[[]]
y = msleep['sleep_total']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 314)
                                                       
scaler = StandardScaler()

X_train_pp = X_train.copy()
X_test_pp = X_test.copy()

col_names = []
scaler.fit(X_train_pp[col_names])
X_train_pp[col_names] = scaler.transform(X_train_pp[col_names])

X_test_pp[col_names] = scaler.transform(X_test_pp[col_names])
```

Now that our data is pre-processed and ready to go, let's build a few models! First, build a multiple linear regression model without any regularization. Calculate the training and testing MSE.

```{python model, exercise = TRUE, message = FALSE, exercise.setup="setup2"}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model_ols = ___
model_ols.fit(___)
print("Coefficients: ", ___)
print("Intercept: ", ___)

train_preds = ___
test_preds = ___

train_mse = ___
test_mse = ___

print("MSE on training set: ", ___)
print("MSE on testing set: ", ___)
```

```{python model-solution, message = FALSE, warning = FALSE, echo = FALSE}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model_ols = LinearRegression()
model_ols.fit(X_train_pp, y_train)
print("Coefficients: ", model_ols.coef_)
print("Intercept: ", model_ols.intercept_)

train_preds = model_ols.predict(X_train_pp)
test_preds = model_ols.predict(X_test_pp)

train_mse = mean_squared_error(y_train,train_preds)
test_mse = mean_squared_error(y_test,test_preds)

print("MSE on training set: ", train_mse)
print("MSE on testing set: ", test_mse)
```

```{r model-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{python setup3, exercise = FALSE, message = FALSE, warning = FALSE, echo = FALSE, exercise.setup="setup2"}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model_ols = LinearRegression()
model_ols.fit(X_train_pp, y_train)

train_preds = model_ols.predict(X_train_pp)
test_preds = model_ols.predict(X_test_pp)

train_mse = mean_squared_error(y_train,train_preds)
test_mse = mean_squared_error(y_test,test_preds)
```

Let's test the remaining MLR assumptions by making diagnostic plots of the residuals.

```{python diagplots, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup3"}
model_df = pd.DataFrame({'Fitted': ___, 'Residuals': ___})

(p9.ggplot(model_df, p9.aes(x = 'Fitted', y = 'Residuals')) +
 p9.geom_point() +
 p9.xlab("Predicted Price") +
 p9.ylab("Residuals"))

(p9.ggplot(model_df, p9.aes(x = 'Residuals')) +
   p9.geom_histogram(bins = 24) +
   p9.xlab("Residuals"))
```

```{python diagplots-solution, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup3"}
model_df = pd.DataFrame({'Fitted': test_preds, 'Residuals': (y_test - test_preds)})

(p9.ggplot(model_df, p9.aes(x = 'Fitted', y = 'Residuals')) +
 p9.geom_point() +
 p9.xlab("Predicted Price") +
 p9.ylab("Residuals"))

(p9.ggplot(model_df, p9.aes(x = 'Residuals')) +
   p9.geom_histogram(bins = 24) +
   p9.xlab("Residuals"))
```

```{r diagplots-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q6, echo=FALSE}
question("True or False: The linear model is appropriate for this dataset.",
         answer("True"),
         answer("False", correct=TRUE),
         allow_retry = TRUE)
```

Next, I want to see if regularization works well on this dataset. Use 5-fold cross validation to choose the best value of $\alpha$ for LASSO and Ridge Regression. What is the overall training and testing MSE for each model? 

```{python model2, exercise = TRUE, message = FALSE, exercise.setup="setup3"}
from sklearn.linear_model import LassoCV, RidgeCV

model_lasso = ___
model_ridge = ___

model_lasso.fit(___)
print("Best Alpha for LASSO: ", ___)

model_ridge.fit(___)
print("Best Alpha for Ridge Regression: ", ___)

train_preds_lasso = ___
test_preds_lasso = ___

train_mse_lasso = ___
test_mse_lasso = ___

print("LASSO - MSE on training set: ", ___)
print("LASSO - MSE on testing set: ", ___)

train_preds_ridge = ___
test_preds_ridge = ___

train_mse_ridge = ___
test_mse_ridge = ___

print("Ridge - MSE on training set: ", ___)
print("Ridge - MSE on testing set: ", ___)
```

```{python model2-solution, message = FALSE, warning = FALSE, echo = FALSE}
from sklearn.linear_model import LassoCV, RidgeCV

model_lasso = LassoCV(cv=5)
model_ridge = RidgeCV(cv=5)

model_lasso.fit(X_train_pp, y_train)
print("Best Alpha for LASSO: ", model_lasso.alpha_)

model_ridge.fit(X_train_pp, y_train)
print("Best Alpha for Ridge Regression: ", model_ridge.alpha_)

train_preds_lasso = model_lasso.predict(X_train_pp)
test_preds_lasso = model_lasso.predict(X_test_pp)

train_mse_lasso = mean_squared_error(y_train,train_preds_lasso)
test_mse_lasso = mean_squared_error(y_test,test_preds_lasso)

print("LASSO - MSE on training set: ", train_mse_lasso)
print("LASSO - MSE on testing set: ", test_mse_lasso)

train_preds_ridge = model_ridge.predict(X_train_pp)
test_preds_ridge = model_ridge.predict(X_test_pp)

train_mse_ridge = mean_squared_error(y_train,train_preds_ridge)
test_mse_ridge = mean_squared_error(y_test,test_preds_ridge)

print("Ridge - MSE on training set: ", train_mse_ridge)
print("Ridge - MSE on testing set: ", test_mse_ridge)
```

```{r model2-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

Which of these 3 models would you choose? Why?
::::

### Modeling `conservation`

Let's switch gears and look at a different variable. Does an animal's sleeping habits affect whether or not it is endangered? Perhaps animals that are asleep for longer periods of time are more vulnerable to poaching or predation...


```{r q7, echo=FALSE}
question(
  "What type of model should I use to answer this question?",
  answer("Supervised", correct = TRUE, message = "That's right! Since we have values for a response variable that we can use to build the model and calculate a loss/error, we would use supervised learning."),
  answer("Unsupervised", message = "Not quite. Unsupervised learning does not involve a response variable."),
  allow_retry = TRUE
)
```

```{r q8, echo=FALSE}
question_checkbox(
  "More specifically, which model(s) could I use?",
  answer("KNN",correct = TRUE,  message = "Correct! KNN (K Nearest Neighbors) is a classification algorithm which allows us to model binary response variables."),
  answer("Simple linear regression", message = "Not quite. Simple linear regression allows us to model a continuous response variable, not a binary response variable"),
  answer("Multiple linear regression", message = "Not quite. Multiple linear regresison will allow us to model a continuous response variable using multiple features, but our repsonse variable is binary."),
  answer("Logistic regression", correct = TRUE, message = "Correct! Logistic regression is a classification algorithm which allows us to model binary response variables."),
  answer("K-Means", message = "Not quite. K-Means is a method of clustering which is an unsupervised learning algorithm."),
  allow_retry = TRUE
)
```

Data prep here


Try running KNN with different numbers of neighbors. Plot the training and the testing accuracy across different values of K.

```{python loop, exercise = TRUE, message = FALSE, exercise.setup="setup4"}
from sklearn.neighbors import KneighborsClassifier

metric_train = []
metric_test = []

for k in range(1, 16):
  tempknn = KNeighborsClassifier(___).fit(___)
  tempknn_preds_train = tempknn.predict(___) 
  tempknn_preds_test = tempknn.predict(___) 
  tempknn_cm_train = pd.crosstab(tempknn_preds_train.tolist(), ___)
  tempknn_cm_test = pd.crosstab(tempknn_preds_train.tolist(), ___)
  metric_train.append((tempknn_cm_train.iloc[0,0] + tempknn_cm_train.[1,1])/X_train.shape[0])
  metric_test.append((tempknn_cm_test.iloc[0,0] + tempknn_cm_test.[1,1])/X_test.shape[0])
  
d_kNN = {'k': [range(1, 16), range(1,16)], 'Accuracy': [metric_train, metric_test], 'Data': (['Train']*15).append(['Test']*15)}
kNN_plot = pd.DataFrame(data = d_kNN)  

print(p9.ggplot(___, p9.aes(x = ___, y = ___, c = ___)) +
       p9.geom_line() +
       p9.geom_vline(xintercept = k) + 
       p9.scale_x_continuous(name = "$k$") + 
       p9.scale_y_continuous(name = "Accuracy")

```

```{python loop-solution, message = FALSE, warning = FALSE, echo = FALSE}
from sklearn.neighbors import KneighborsClassifier

metric_train = []
metric_test = []

for k in range(1, 16):
  tempknn = KNeighborsClassifier(n_neighbors = k).fit(X_train_pp,y_train)
  tempknn_preds_train = tempknn.predict(X_train_pp) 
  tempknn_preds_test = tempknn.predict(X_test_pp) 
  tempknn_cm_train = pd.crosstab(tempknn_preds_train.tolist(), y_train)
  tempknn_cm_test = pd.crosstab(tempknn_preds_train.tolist(), y_test)
  metric_train.append((tempknn_cm_train.iloc[0,0] + tempknn_cm_train.[1,1])/X_train.shape[0])
  metric_test.append((tempknn_cm_test.iloc[0,0] + tempknn_cm_test.[1,1])/X_test.shape[0])
  
d_kNN = {'k': [range(1, 16), range(1,16)], 'Accuracy': [metric_train, metric_test], 'Data': (['Train']*15).append(['Test']*15)}
kNN_plot = pd.DataFrame(data = d_kNN)  

print(p9.ggplot(kNN_plot, p9.aes(x = "k", y = "Accurracy", c = "Data")) +
       p9.geom_line() +
       p9.geom_vline(xintercept = k) + 
       p9.scale_x_continuous(name = "$k$") + 
       p9.scale_y_continuous(name = "Accuracy")
```

```{r loop-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

* What value of K would you choose? Why?
* For which values of K does the model underfit? Overfit?

::::


```{r q9, echo=FALSE}
question_numeric("What $k$? would you choose?",
         answer_fn(function(value){
           if (value %in% c()){
             correct("Good choice!")
           }
           else{
             incorrect("Not quite. Remember we want testing error to be low (or, equivalently, testing accuracy to be high).")
           }
         }),
         allow_retry = TRUE, 
         post_message = "Congratulations! You have found the fifth and final secret word: SOUP."
)
```

Let's see if we can get better accuracy with logistic regression.

```{python logit, exercise = TRUE, message = FALSE, exercise.setup="setup4"}
from sklearn.linear_model import LogisticRegression

logit_model = ___
logit_model.fit(___)

test_preds = logit_model.predict(___)

logit_cm = pd.crosstab(___)
print("Testing accuracy: ",(logit_cm.iloc[___] + logit_cm.iloc[___])/X_train.shape[0])

```

```{python logit-solution, exercise = TRUE, message = FALSE, exercise.setup="setup4"}
from sklearn.linear_model import LogisticRegression

logit_model = LogisticRegression()
logit_model.fit(X_train_pp, y_train)

test_preds = logit_model.predict(X_test_pp)

logit_cm = pd.crosstab(test_preds.tolist(), y_test)
print("Testing accuracy: ",(logit_cm.iloc[0,0] + logit_cm.iloc[1,1])/X_train.shape[0])

```

```{r logit-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{python setup5, exercise = FALSE, message = FALSE, exercise.setup="setup4"}
from sklearn.linear_model import LogisticRegression

logit_model = LogisticRegression()
logit_model.fit(X_train_pp, y_train)

test_preds = logit_model.predict(X_test_pp)

logit_cm = pd.crosstab(test_preds.tolist(), y_test)
print("Testing accuracy: ",(logit_cm.iloc[0,0] + logit_cm.iloc[1,1])/X_train.shape[0])

```


Now, print the AUC on the test set. How well does the model perform?

```{python auc, exercise = TRUE, message = FALSE, exercise.setup="setup5"}
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = metrics.roc_curve(___)
metrics.auc(___, ___)
```

```{python auc-solution, message = FALSE, warning = FALSE, echo = FALSE}
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_test, test_preds, pos_label=1)
metrics.auc(fpr, tpr)
```

```{r auc-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q10, echo=FALSE}
question("True or False: The test set AUC for this model represents an improvement over random chance.",
         answer("True", correct=TRUE),
         answer("False", message = "Don't forget, we want AUC to be above 0.5!"),
         allow_retry = TRUE, 
         post_message = "Congratulations! You have found the fifth and final secret word: PRINCIPLE."
)
```

Now, let's plot the ROC curve. In a previous lab, we did this by hand, but `sklearn` has a function that will generate this plot for us. Run the code below to see the ROC curve,

```{python roc_curve, exercise = TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup5"}
from sklearn.metrics import RocCurveDisplay

fpr, tpr, thresholds = roc_curve(y_test, test_preds, pos_label=1)
roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()
plt.show()

```

Using the cell below to explore the model further...

```{python logit_play, exercise = TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup5"}


```

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

* Which variables are most related to conservation? 
* Interpret the coefficients of the model.

::::

### Clustering

Lastly, I'm curious if there are groups of animals who have similar features and whether or not these clusters are reflective of the greater taxonomy (using `genus` and `order`).

```{r q11, echo=FALSE}
question(
  "What type of model should I use to answer this question?",
  answer("Supervised", message = "Not quite. Since we are looking for groups with similar featuers, we want to use a clustering method. In clustering, there is no response variable."),
  answer("Unsupervised",correct = TRUE, message = "Correct!"),
  allow_retry = TRUE
)
```

In class, we learned about 2 clustering methods K-Means and Hierarchical clustering. 

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

Which clustering method do you think makes the most sense for this dataset?

::::

First, let's use K-Means to cluster the data. We need to prepare our data for clustering.
- Read in the data with `name` as the index
- Create dummy variables for the categorical variables 
- Standardize the numeric variables

Remember, that we don't need to split data into training and testing for unsupervised learning!

```{python setup6, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
from sklearn.cluster import KMeans
msleep = pd.read_csv("./data/msleep.csv", index_col = "name")

msleep = pd.get_dummies(msleep, columns = ['vore', 'conservation'], drop_first = True)

scaler = StandardScaler()

X_train_pp = X_train.copy()
X_test_pp = X_test.copy()

col_names = []
msleep[col_names] = scaler.fit_transform(msleep[col_names])
```

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

Why do we need to standardize our numeric variables before we use our clustering algorithms?

::::

Next, create an elbow plot to help us choose the best value for K.

```{python looped, exercise = TRUE, message = FALSE, exercise.setup="setup6"}
inertias = []

for i in range(1,16):
    kmeans = KMeans(n_clusters=i, n_init = 5, random_state = 314)
    kmeans.fit(___)
    inertias.append(...)
    
chooseK = {'K': ___, 'Inertia': ___}
chooseK_df = pd.DataFrame(data = ___)

print(p9.ggplot(___, p9.aes(x = ___, y = ___)) +
       p9.geom_line() +
       p9.scale_x_continuous(name = "$K$") + 
       p9.scale_y_continuous(name = "Inertia") +
       p9.theme(legend_position = "none", figure_size = [6, 3.5]))
```

```{python looped-solution, message = FALSE, warning = FALSE, echo = FALSE}
inertias = []

for i in range(1,16):
    kmeans = KMeans(n_clusters=i, n_init = 5, random_state = 314)
    kmeans.fit(epi)
    inertias.append(kmeans.inertia_)
    
chooseK = {'K': range(1, 16), 'Inertia': inertias}
chooseK_df = pd.DataFrame(data = chooseK)

print(p9.ggplot(chooseK_df, p9.aes(x = 'K', y = 'Inertia')) +
       p9.geom_line() +
       p9.scale_x_continuous(name = "$K$") + 
       p9.scale_y_continuous(name = "Inertia") +
       p9.theme(legend_position = "none", figure_size = [6, 3.5]))
```

```{r looped-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q12, echo=FALSE}
question_numeric("What $k$? would you choose?",
         answer_fn(function(value){
           if (value %in% c()){
             correct("Good choice!")
           }
           else{
             incorrect("Not quite. Remember we are looking for the point of diminshing returns.")
           }
         }),
         allow_retry = TRUE, 
         post_message = "Congratulations! You have found the fifth and final secret word: SOUP."
)
```

Rerun the clustering using this value of $k$. (Note that the solution may use a different value of k from what you chose - if your $k$ passed the question above, it was still a good choice of $k$!) For each resulting cluster print the animals as well as their order and genus.

```{python kmeans, exercise = TRUE, message = FALSE, exercise.setup = "setup6"}
kmeans = KMeans(n_clusters = ___, n_init = 5, random_state = 314)
clust = kmeans.fit(___)

msleep[clust.labels_ == 0.0]
```

```{python kmeans-solution, exercise = TRUE, message = FALSE, exercise.setup = "setup6"}
kmeans = KMeans(n_clusters = ___, n_init = 5, random_state = 314)
clust = kmeans.fit(msleep)

msleep[clust.labels_ == 0.0]
```

```{r kmeans-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{python setup7, message = FALSE, echo = FALSE, exercise.setup = "setup6"}
kmeans = KMeans(n_clusters = ___, n_init = 5, random_state = 314)
clust = kmeans.fit(msleep)

msleep[clust.labels_ == 0.0]
```

Use this cell to print and view the clusters:

```{python clust_view1, exercise = TRUE, exercise.eval= False, message = FALSE, exercise.setup = "setup7"}
msleep[clust.labels_ == ___]
```

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

Do you notice a pattern between clusters and the `order` and `genus` values?

::::

Now, let's use hierarchical clustering. Try a few different linkage and dissimilarity metrics and choose the one that creates the most balanced dendrogram. (The solution for this cell is just one of may potential choices. What we want you to do here is explore multiple choices and choose your favorite.)

For each cluster print the animals as well as their order and genus. 

```{python hier, exercise = TRUE, message = FALSE, exercise.setup = "setup7"}
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

link = linkage(___)
dendrogram(link, no_labels = False, above_threshold_color='k')
```

```{python hier-solution, exercise = TRUE, message = FALSE, exercise.setup = "setup7"}
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

link = linkage(msleep, method='complete')
dendrogram(link, no_labels = False, above_threshold_color='k')
```

```{r hier-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{python setup8, message = FALSE, echo = FALSE, exercise.setup = "setup7"}
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

link = linkage(msleep, method='complete')
dendrogram(link, no_labels = False, above_threshold_color='k')
```

Use this cell to print and view the clusters:

```{python clust_view, exercise = TRUE, exercise.eval= False, message = FALSE, exercise.setup = "setup8"}
clust_hier = fcluster(link, t=0.9, criterion='distance')
msleep[clust_hier == ___]
```

:::: {.discussionbox data-latex=""}
::: {.center data-latex=""}
**Discuss with a neighbor (or on Ed):**
:::

* How about now? Do you notice a pattern between clusters and the `order` and `genus` values?
* Which clustering method do you feel produced the best clustering?
* Which clustering method produced a clustering that best separated the animals based on their order and genus?

::::


