---
title: "Data119 - Lab 9"
output: 
   learnr::tutorial:
      css: css/custom-styles.css
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)

library(shiny)
library(ggplot2)
library(glmnet)
library(dplyr)
library(tidyr)
library(DBI)
library(gradethis)
library(learnr)
library(reticulate)
library(RSQLite)

goodreads_nussbaum <- read.csv("./data/goodreads_nussbaum.csv")
colnames(goodreads_nussbaum) <- c("book_id", "title", "author", "author_lf", 
                                  "additional_authors", "isbn", "isbn13",
                                  "my_rating", "average_rating", "publisher",
                                  "binding", "number_of_pages",
                                  "year_published",
                                  "original_year_published", "date_read", 
                                  "date_added", "bookshelves", 
                                  "bookshelves_with_positions",
                                  "exclusive_shelf", "my_review", "spoilers",
                                  "private_notes", "read_count",
                                  "owned_copies")
locations <- read.csv("./data/locations.csv", row.names = 1)
colnames(locations) <- c("location", "title")

db_con <- DBI::dbConnect(RSQLite::SQLite(), dbname = ":memory:")
goodreads_nussbaum <- DBI::dbWriteTable(db_con, "goodreads_nussbaum", goodreads_nussbaum)
locations <- DBI::dbWriteTable(db_con, "locations", locations)

# Set the path to the existing Python environment
#reticulate::use_python("/opt/python/3.9.21/bin/python", required = TRUE)

# Optional: Install necessary Python packages if not already installed
# reticulate::py_install(c('numpy', 'pandas', 'plotnine'))

custom_checker <- function(label, user_code, solution_code, envir_result, evaluate_result, last_value, stage, ...) {
  if (stage == "code_check") {
      if (user_code == solution_code){
          return(list(message = random_praise(), correct = TRUE))
      }
    return(list(message = random_encouragement(), correct = FALSE))
  }
}

gradethis_setup()
tutorial_options(exercise.completion = FALSE, exercise.checker = custom_checker)
```

```{r header, echo = FALSE}
library(htmltools)

tags$div(
  class = "topContainer",
  tags$div(
    class = "logoAndTitle",
    tags$img(
      src = "./images/dsi_logo.png",
      alt = "DSI Logo",
      class = "topLogo"
    ),
    tags$h1("Data 119 Review", class = "pageTitle")
  )
)
```

## Goals

The goal of this lab is to help you review topics we've covered this quarter.

## Setup

For this lab we will be using `plotnine`, `pandas`, `numpy`, `scikit-learn`, `scipy`, and the dataset `epi_r`. 

```{python setup-packages, exercise=TRUE, exercise.eval = FALSE, message = FALSE}
import numpy as np
import pandas as pd
import plotnine as p9
import scipy
import sklearn

# To download this file go to https://posit.ds.uchicago.edu/data119-lab9/www/epi_mini2.csv

epi = pd.read_csv("./data/epi_mini2.csv", index_col = 0)
```


## Concept Review

In the next few sections, you will review what we learned in class by using a few interactive widgets. Investigate the best choices of models, variables, transformations, and hyperparameters and then answer the questions that follow.

### Reviewing regression

Use the widget below to review what we learned about multiple linear regression:

```{r regression_app, echo = FALSE}
# Generate synthetic data
set.seed(123)
n <- 200
x1 <- rnorm(n, 2, 2)
x2 <- rexp(n)
x3 <- rnorm(n)
x4 <- rbinom(n,7,.3)
x5 <- rnorm(n,10,2)
y <- (3 + 2*x1 - 1.5*x2 + 0.5*x3 + x5)^2 + x3^4 + 6*x3*x5 + x5^3 + rnorm(n)

data <- data.frame(x1, x2, x3, x4, x5, y)

# UI
ui <- fluidPage(
  titlePanel("Regularized Regression with Transformations & Polynomial Features"),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("reg_method", "Regularization Type:", choices = c("Ridge", "Lasso")),
      checkboxGroupInput("features", "Select Features:", 
                         choices = names(data)[-6], 
                         selected = names(data)[-6]),
      
      selectInput("transform", "Transform Response Variable (y):", choices = c("None", "Log", "Sqrt"), selected = "None"),
      sliderInput("poly_degree", "Degree of Polynomial Features:", min = 1, max = 5, value = 1),
      
      sliderInput("alpha", "Regularization Parameter (α):", min = 0, max = 100, value = 0, step = 0.01)
    ),
    
    mainPanel(
      h3("Model Performance"),
      tableOutput("metrics"),
      
      h3("Coefficient Paths"),
      plotOutput("coef_plot"),
      
      h3("Residual Plot"),
      plotOutput("residual_plot"),
      
      h3("Residual Distribution"),
      plotOutput("residual_dist")
    )
  )
)

# Server
server <- function(input, output) {
  
  model_data <- reactive({
    req(input$features)  # Ensure at least one feature is selected
    df <- data %>% select(all_of(input$features), y)
    
    # Ensure y is numeric
    df$y <- as.numeric(df$y)
    
    # Apply transformation safely
    if (input$transform == "Log") {
      if (min(df$y) <= 0) df$y <- df$y - min(df$y) + 1  # Shift to positive
      df$y <- log(df$y)
    } else if (input$transform == "Sqrt") {
      if (min(df$y) < 0) df$y <- df$y - min(df$y) + 1  # Shift to non-negative
      df$y <- sqrt(df$y)
    }
    
    # Apply polynomial features properly
    if (input$poly_degree > 1) {
      poly_data <- df %>% select(-y)  # Select only predictor columns
      poly_expanded <- lapply(poly_data, function(col) {
        as.data.frame(poly(col, degree = input$poly_degree, raw = TRUE))  # Convert each column separately
      })
      
      poly_df <- bind_cols(poly_expanded)  # Merge polynomial features
      colnames(poly_df) <- paste0(rep(names(poly_data), each = input$poly_degree), "_poly", 1:input$poly_degree)
      
      df <- cbind(poly_df, y = df$y)  # Merge back with y
    }
    
    return(df)
  })
  
  fit_model <- reactive({
    req(length(input$features) > 0)  # Ensure at least one feature is selected
    
    df <- model_data()
    x <- as.matrix(df %>% select(-y))
    y <- df$y
    
    lambda_value <- input$alpha  # Alpha is now the regularization penalty
    alpha_value <- ifelse(input$reg_method == "Ridge", 0, 1) # Ridge = 0, Lasso = 1
    model <- glmnet(x, y, alpha = alpha_value, lambda = lambda_value)
    
    return(list(model = model, x = x, y = y))
  })
  
  output$metrics <- renderTable({
    res <- fit_model()
    model <- res$model
    lambda_value <- input$alpha
    
    preds <- predict(model, newx = res$x, s = lambda_value)
    mse <- mean((res$y - preds)^2)
    r_squared <- 1 - (mse / var(res$y))
    
    data.frame(
      Metric = c("MSE", "R-squared"),
      Value = c(round(mse, 4), round(r_squared, 4))
    )
  })
  
  output$coef_plot <- renderPlot({
    req(input$reg_method %in% c("Ridge", "Lasso"))  # Ensure Ridge or Lasso is selected
    
    res <- fit_model()
    alpha_value <- ifelse(input$reg_method == "Ridge", 0, 1) # Ridge = 0, Lasso = 1
    
    # Fit model across different values of lambda
    lambda_seq <- exp(seq(log(0.001), log(100), length.out = 50))  # Log-spaced λ values
    
    coef_paths <- lapply(lambda_seq, function(l) {
      model <- glmnet(res$x, res$y, alpha = alpha_value, lambda = l)
      coefs <- as.matrix(coef(model))[-1, , drop = FALSE]  # Ensure correct dimension
      data.frame(Feature = rownames(coefs), Coefficient = as.vector(coefs), Lambda = l)
    })
    
    coef_df <- do.call(rbind, coef_paths)  # Merge into one dataframe
    
    best_lambda <- input$alpha  # Now alpha is used as the regularization penalty
    
    ggplot(coef_df, aes(x = Lambda, y = Coefficient, color = Feature)) +
      geom_line() +
      geom_vline(xintercept = best_lambda, linetype = "dashed", color = "red") +
      scale_x_log10() +
      labs(title = paste(input$reg_method, "Coefficient Paths Across Lambda Values"),
           x = "Lambda (Regularization Penalty)", y = "Coefficient") +
      theme_minimal()
  })
  
  output$residual_plot <- renderPlot({
    res <- fit_model()
    model <- res$model
    preds <- as.vector(predict(model, newx = res$x, s = input$alpha))  # Ensure correct dimension
    residuals <- res$y - preds
    
    ggplot(data.frame(Predicted = preds, Residuals = residuals), aes(x = Predicted, y = Residuals)) +
      geom_point(alpha = 0.7, color = "blue") +
      geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
      theme_minimal() +
      labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals")
  })
  
  output$residual_dist <- renderPlot({
    res <- fit_model()
    model <- res$model
    preds <- as.vector(predict(model, newx = res$x, s = input$alpha))  # Ensure correct dimension
    residuals <- res$y - preds
    
    ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
      geom_histogram(aes(y = ..density..), bins = 20, fill = "blue", alpha = 0.6) +
      geom_density(color = "red", linewidth = 1) +
      theme_minimal() +
      labs(title = "Residual Distribution", x = "Residuals", y = "Density")
  })
  
}

# Run the app
shinyApp(ui = ui, server = server)
```

Questions go here

### Reviewing Classification

Use the widget below to review what we learned about classification:

```{r classification_app, echo=FALSE}
library(shiny)
library(ggplot2)
library(caret)
library(pROC)
library(dplyr)

# Generate synthetic data
set.seed(123)
n <- 200
x1 <- rnorm(n)
x2 <- rnorm(n)
prob <- 1 / (1 + exp(- (0.5 * x1 + 0.8 * x2))) # Logistic function
y <- rbinom(n, 1, prob) # Generate binary labels

data <- data.frame(x1, x2, y)

# Fit logistic regression model
model <- glm(y ~ x1 + x2, data = data, family = "binomial")
data$predicted_prob <- predict(model, type = "response")

# UI
ui <- fluidPage(
  titlePanel("Threshold Effect on Classification Metrics"),
  
  sidebarLayout(
    sidebarPanel(
      sliderInput("threshold", "Threshold:", min = 0, max = 1, value = 0.5, step = 0.01),
      h3("ROC Curve"),
      plotOutput("roc_curve", height = "300px") # ROC curve directly below the slider
    ),
    
    mainPanel(
      h3("Confusion Matrix"),
      htmlOutput("conf_matrix"), # Confusion matrix with bold labels
      
      h3("Model Evaluation Metrics"),
      tableOutput("metrics"),
      
      h3("Data (First 10 Rows)"),
      tableOutput("data_snippet")  # New: Data table snippet
    )
  )
)

# Server
server <- function(input, output) {
  
  pred_class <- reactive({
    ifelse(data$predicted_prob >= input$threshold, 1, 0)
  })
  
  confusion_stats <- reactive({
    cm <- confusionMatrix(factor(pred_class()), factor(data$y))
    as.matrix(cm$table)
  })
  
  output$conf_matrix <- renderUI({
    cm <- confusion_stats()
    HTML(sprintf("
      <table class='table table-bordered' style='text-align:center; font-size:16px;'>
        <tr>
          <th></th>
          <th>Predicted 0</th>
          <th>Predicted 1</th>
        </tr>
        <tr>
          <td><b>Actual 0</b></td>
          <td>%d</td>
          <td>%d</td>
        </tr>
        <tr>
          <td><b>Actual 1</b></td>
          <td>%d</td>
          <td>%d</td>
        </tr>
      </table>
    ", cm[1,1], cm[1,2], cm[2,1], cm[2,2]))
  })
  
  output$metrics <- renderTable({
    cm <- confusion_stats()
    TN <- cm[1, 1]
    FP <- cm[1, 2]
    FN <- cm[2, 1]
    TP <- cm[2, 2]
    
    sensitivity <- TP / (TP + FN)
    specificity <- TN / (TN + FP)
    accuracy <- (TP + TN) / (TP + TN + FP + FN)
    
    data.frame(
      Metric = c("Sensitivity (Recall)", "Specificity", "Accuracy"),
      Equation = c("TP / (TP + FN)", "TN / (TN + FP)", "(TP + TN) / (TP + TN + FP + FN)"),
      Computation = c(
        sprintf("%d / (%d + %d)", TP, TP, FN),
        sprintf("%d / (%d + %d)", TN, TN, FP),
        sprintf("(%d + %d) / (%d + %d + %d + %d)", TP, TN, TP, TN, FP, FN)
      ),
      Value = c(
        sprintf("%.4f", sensitivity),
        sprintf("%.4f", specificity),
        sprintf("%.4f", accuracy)
      )
    )
  }, align = "lccc", digits = 4)
  
  output$data_snippet <- renderTable({
    snippet <- data %>%
      mutate(Prediction = ifelse(predicted_prob >= input$threshold, 1, 0)) %>%
      select(x1, x2, predicted_prob, Prediction) %>%
      head(10)
    
    colnames(snippet) <- c("X1", "X2", "Predicted Probability", "Prediction")
    snippet
  }, digits = 4)
  
  output$roc_curve <- renderPlot({
    roc_obj <- roc(data$y, data$predicted_prob)
    ggplot(data.frame(tpr = roc_obj$sensitivities, fpr = 1 - roc_obj$specificities), aes(x = fpr, y = tpr)) +
      geom_line() +
      geom_abline(linetype = "dashed") +
      geom_point(aes(x = 1 - roc_obj$specificities[which.min(abs(roc_obj$thresholds - input$threshold))], 
                     y = roc_obj$sensitivities[which.min(abs(roc_obj$thresholds - input$threshold))]), 
                 color = "red", size = 3) +
      labs(x = "False Positive Rate", y = "True Positive Rate", title = "ROC Curve") +
      theme_minimal()
  })
}

# Run the app
shinyApp(ui = ui, server = server)
```

Questions go here

### Reviewing Clustering

Use the widget below to review what we learned about clustering:

```{r clustering_app, echo=FALSE}
library(shiny)
library(ggplot2)
library(cluster)
library(dendextend)

# Generate synthetic data
set.seed(123)
n <- 200
x <- rnorm(n)
y <- rnorm(n)
data <- data.frame(x, y)

# UI
ui <- fluidPage(
  titlePanel("Clustering Exploration"),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("clust_method", "Clustering Method:", choices = c("K-Means", "Hierarchical")),
      
      # Options for Hierarchical Clustering
      conditionalPanel(
        condition = "input.clust_method == 'Hierarchical'",
        selectInput("dist_metric_hc", "Distance Metric:", choices = c("euclidean", "manhattan")),
        selectInput("linkage", "Linkage Method:", choices = c("complete", "single", "average", "centroid")),
        sliderInput("cut_height", "Dendrogram Cut Height:", min = 0.5, max = 5, value = 2, step = 0.1)
      ),
      
      # Options for K-Means Clustering
      conditionalPanel(
        condition = "input.clust_method == 'K-Means'",
        selectInput("dist_metric_km", "Distance Metric:", choices = c("euclidean", "manhattan")),
        sliderInput("k_value", "Number of Clusters (k):", min = 2, max = 10, value = 3, step = 1)
      )
    ),
    
    mainPanel(
      conditionalPanel(
        condition = "input.clust_method == 'Hierarchical'",
        plotOutput("dendrogram"),
        plotOutput("hier_scatter")
      ),
      conditionalPanel(
        condition = "input.clust_method == 'K-Means'",
        plotOutput("elbow_plot"),
        plotOutput("kmeans_scatter")
      )
    )
  )
)

# Server
server <- function(input, output) {
  
  # Compute hierarchical clustering distance matrix
  dist_matrix_hc <- reactive({
    dist(data, method = input$dist_metric_hc)
  })
  
  # Compute hierarchical clustering
  hc <- reactive({
    hclust(dist_matrix_hc(), method = input$linkage)
  })
  
  # Render dendrogram with smaller labels
  output$dendrogram <- renderPlot({
    dend <- as.dendrogram(hc())
    dend <- color_branches(dend, h = input$cut_height)
    dend <- set(dend, "labels_cex", 0.5)  # This makes labels smaller
    plot(dend, main = "Hierarchical Clustering Dendrogram",)
    abline(h = input$cut_height, col = "red", lty = 2)
  })
  
  # Render scatterplot for hierarchical clustering
  output$hier_scatter <- renderPlot({
    clusters <- cutree(hc(), h = input$cut_height)
    ggplot(data, aes(x, y, color = factor(clusters))) +
      geom_point(size = 3) +
      labs(title = "Hierarchical Clustering", color = "Cluster") +
      theme_minimal()
  })
  
  # Compute distance matrix for K-means clustering
  kmeans_data <- reactive({
    if (input$dist_metric_km == "euclidean") {
      return(data)
    } else {
      return(as.matrix(dist(data, method = "manhattan")))  # Convert Manhattan to a format K-means can use
    }
  })
  
  # Compute K-means clustering
  kmeans_result <- reactive({
    kmeans(kmeans_data(), centers = input$k_value, nstart = 10)
  })
  
  # Render K-means scatterplot
  output$kmeans_scatter <- renderPlot({
    ggplot(data, aes(x, y, color = factor(kmeans_result()$cluster))) +
      geom_point(size = 3) +
      labs(title = "K-Means Clustering", color = "Cluster") +
      theme_minimal()
  })
  
  # Compute WCSS (Within-cluster sum of squares) for Elbow plot
  wcss <- reactive({
    sapply(1:10, function(k) kmeans(kmeans_data(), centers = k, nstart = 10)$tot.withinss)
  })
  
  # Render elbow plot
  output$elbow_plot <- renderPlot({
    elbow_df <- data.frame(k = 1:10, wcss = wcss())
    ggplot(elbow_df, aes(x = k, y = wcss)) +
      geom_line() +
      geom_point() +
      geom_point(aes(x = input$k_value, y = wcss()[input$k_value]), color = "red", size = 4) +
      labs(title = "Elbow Method for K-Means", x = "Number of Clusters (k)", y = "WCSS") +
      theme_minimal()
  })
}

# Run the app
shinyApp(ui = ui, server = server)

```

Questions go here

## Implementation Review

### Modeling `regression_var`
- get data with sql
- have them decide what type of model to use
- 

### Modeling `classification_var`
- different data out of sql
- what model

### Clustering
- different data out of sql
- what model

```{r q11a, echo=FALSE}
question(
  "How many rows are there in `locations`?",
  answer("1000"),
  answer("51755"), 
  answer("120775", correct = TRUE),
  allow_retry = TRUE,
  post_message = "The way our labs are set up does not allow us to view more than 1000 rows! But there are 51,755 distinct titles and 120,775 entries in the table--experiment with `COUNT()` and `DISTINCT()` to see if you can recreate these numbers."
)
```

```{r q11b, echo=FALSE}
question_numeric(
  "How many variables are there in `locations`?",
  answer(2, correct = TRUE),
  allow_retry = TRUE,
  min = 0,
  max = 10,
  step = 1,
  post_message = "Congratulations! You have found the fourth secret word: TRUNK."
)
```

1.  Read through the columns to try and get a sense of what each variable means. Then, write a query that will return a table with the counts of Professor Nussbaum's books taking place in each continent.

```{sql goodreads12, exercise = TRUE, connection = "db_con"}

```

```{sql goodreads12-solution, message = FALSE, warning = FALSE, echo = FALSE}
SELECT location, COUNT(location)
FROM locations
WHERE location IN ("africa", "antarctica", "asia", "europe", "north-america", "oceania", "latin-america")
GROUP BY location;
```

```{r goodreads12-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q12, echo=FALSE}
question(
  "Which continent has the most titles associated with it?",
  answer("`africa`"),
  answer("`antarctica`"),
  answer("`asia`"),  
  answer("`europe`", correct = TRUE),
  answer("`latin-america`"),  
  answer("`north-america`"),  
  answer("`oceania`"),  
  allow_retry = TRUE
)
```

Oddly, there is no Central or South America on this list. It uses Latin America instead–even though I’m not 100% sure those are direct analogs. The locations are open to interpretation, which means something like creating a map, the original task Professor Nussbaum was trying to carry out with this dataset, can be very subjective.

13. Write a query that will return a count of all of the books that are in both `goodread_nussbaum` and `locations`. It may help to abbreviate the tables in your query.

```{sql goodreads13, exercise = TRUE, connection = "db_con"}
SELECT ...
FROM ... as A
JOIN ... as B ON ...;
```

```{sql goodreads13-solution, message = FALSE, warning = FALSE, echo = FALSE}
SELECT COUNT(DISTINCT(A.title))
FROM goodreads_nussbaum as A
JOIN locations as B ON A.title = B.title;
```

```{r goodreads13-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q13a, echo=FALSE}
question(
  "What type of join should we be using to determine how many titles `goodreads_nussbaum` and `locations` have in common?",
  answer("Inner", correct = TRUE),
  answer("Outer"),
  answer("Right"),
  answer("Left"),
  allow_retry = TRUE
)
```

```{r q13b, echo=FALSE}
question_numeric(
  "How many titles occur in both lists?",
  answer(166, correct = TRUE),
  min = 0,
  max = 200,
  step = 1,  allow_retry = TRUE
)
```

14. Write a query that will join Professor Nussbaum's books with the `locations` object. It may help to create a new table (call it `both`).

```{sql goodreads14, exercise = TRUE, connection = "db_con"}
...
SELECT *
FROM ... as a
JOIN ... as b ON a.title = b.title;
```

```{sql goodreads14-solution, message = FALSE, warning = FALSE, echo = FALSE}
CREATE TABLE both AS
SELECT *
FROM goodreads_nussbaum as a
JOIN locations as b ON a.title = b.title;
```

```{r goodreads14-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

15. Using `both`, write a query that will return which of Professor Nussbaum’s books take place in Illinois. 

```{sql setup1, exercise = FALSE, echo=FALSE, message = FALSE}
CREATE TABLE both AS
SELECT *
FROM goodreads_nussbaum as a
JOIN locations as b ON a.title = b.title;
```

```{sql goodreads15, exercise = TRUE, connection = "db_con", exercise.setup = "setup1"}

```

```{sql goodreads15-solution, message = FALSE, warning = FALSE, echo = FALSE}
SELECT title, location, exclusive_shelf
FROM both
WHERE exclusive_shelf = "read" and location = "us-illinois";
```

```{r goodreads15-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q15, echo=FALSE}
question_numeric(
  "How many of Professor Nussbaum's books take place in Illinois?",
  answer(4, correct = TRUE),
  min = 0, 
  max = 10, 
  step = 1,
  allow_retry = TRUE
)
```

16. Write a query that will return Professor Nussbaum's most-liked books that have taken place in England. 

```{sql goodreads16, exercise = TRUE, connection = "db_con", exercise.setup = "setup1"}

```

```{sql goodreads16-solution, message = FALSE, warning = FALSE, echo = FALSE}
SELECT title, my_rating, location, exclusive_shelf
FROM both
WHERE exclusive_shelf = "read" and location = "uk-england" AND my_rating = 5;
```

```{r goodreads16-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q16, echo=FALSE}
question(
  "What is one of the books Professor Nussbaum liked best that took place in England?",
  answer("Matilda", correct = TRUE),
  answer("Persuasion"),
  answer("The Girl on the Train"),
  answer("The Weight of Ink"),
  allow_retry = TRUE, 
  post_message = "Congratulations! You have found the fifth and final secret word: DEPUTY."
)
```

17. Adapt your query to return Professor Nussbaum's least-liked books that have taken place in England.

```{sql goodreads17, exercise = TRUE, connection = "db_con", exercise.setup = "setup1"}

```

```{sql goodreads17-solution, message = FALSE, warning = FALSE, echo = FALSE}
SELECT title, my_rating, locations, exclusive_shelf
FROM both
WHERE exclusive_shelf = "read" and locations = "uk-england" AND my_rating = 2;
```

```{r goodreads17-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q17, echo=FALSE}
question(
  "What is one of the books Professor Nussbaum liked least that took place in England?",
  answer("Mansfield Park"),
  answer("The Mists of Avalon", correct = TRUE),
  answer("The Princess Diarist"),
  answer("Treasure Island"),
  allow_retry = TRUE
)
```

18. What is the most highly rated book that takes place in each of Argentina, France, Germany, Japan, Italy, and Switzerland?

```{sql goodreads18, exercise = TRUE, connection = "db_con", exercise.setup = "setup1"}

```

```{sql goodreads18-solution, message = FALSE, warning = FALSE, echo = FALSE}
SELECT title, MAX(average_rating), location
FROM both
WHERE location IN ("argentina", "france", "germany", "japan", "italy", "switzerland")
GROUP BY location;
```

```{r goodreads18-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q18, echo=FALSE}
question(
  "According to this table, where does the book *Pachinko* take place?",
  answer("Argentina"),
  answer("France"),
  answer("Germany"),
  answer("Japan", correct = TRUE),
  answer("Italy"),
  answer("Switzerland"),
  allow_retry = TRUE
)
```

19. When you have finished, use the cell below to write a query that you think answers an interesting question about this dataset. You can also use the cell to [explore more keywords in SQLite](https://www.sqlite.org/lang_keywords.html). Post the question in the thread on Ed so that your classmates can get more practice!

```{sql goodreads19, exercise = TRUE, connection = "db_con", exercise.setup = "setup1"}

```

