---
title: "Data119 - Lab 3"
output: 
   learnr::tutorial:
      css: css/custom-styles.css
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)

library(learnr)
library(gradethis)
library(reticulate)

# Set the path to the existing Python environment
#reticulate::use_python("/opt/python/3.9.21/bin/python", required = TRUE)

# Optional: Install necessary Python packages if not already installed
# reticulate::py_install(c('numpy', 'pandas', 'plotnine'))

custom_checker <- function(label, user_code, solution_code, envir_result, evaluate_result, last_value, stage, ...) {
  if (stage == "code_check") {
      if (user_code == solution_code){
          return(list(message = random_praise(), correct = TRUE))
      }
    return(list(message = random_encouragement(), correct = FALSE))
  }
}

gradethis_setup()
tutorial_options(exercise.completion = FALSE, exercise.checker = custom_checker, exercise.timelimit = 30)
```

```{r header, echo = FALSE}
library(htmltools)

tags$div(
  class = "topContainer",
  tags$div(
    class = "logoAndTitle",
    tags$img(
      src = "./images/dsi_logo.png",
      alt = "DSI Logo",
      class = "topLogo"
    ),
    tags$h1("Assumption Checking and Multiple Linear Regression", class = "pageTitle")
  )
)
```

## Goals

-   Address failed assumptions in linear regression.
-   Explore datasets with multiple variables for the purpose of multiple linear regression.
-   Find significant predictors for a response variable in multiple linear regression.

## Setup

For this lab we will be using `numpy`, `pandas`, `plotnine`, `statsmodels`, and a random sample of the AirBnB dataset you've seen in previous homework assignments. Refresh yourself on the variables the dataset includes by reading the [Kaggle documentation](https://www.kaggle.com/datasets/thedevastator/airbnb-prices-in-european-cities). 

The other thing we want to be able to do is investigate more than a few columns (for example, in a correlation matrix). We can use the `set_option()` function from `pandas` to force Python to show us all of the output. See the last line of code in the cell below for an example. 
Run the cell below to setup our environment.

```{python setup1, exercise=TRUE}
import numpy as np
import pandas as pd
import plotnine as p9
import statsmodels.api as sm

AirBnB_prices = pd.read_csv("./www/AirBnB_mini.csv")

pd.set_option('display.max_columns', None)
```

## Reviewing SLR with `statsmodels`

1. Using `statsmodels` syntax, refit a simple linear regression model predicting `realSum` from `dist`, and make a note of the $R^2$ (which can be found in the `.summary()` output.

```{python slr, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
X1 = AirBnB_prices[...]
X1 = sm.add_constant(X1)

Y = AirBnB_prices[...]

model1 = sm.OLS(Y, X1).fit()
model1.summary()
```

```{python slr-solution, message = FALSE, warning = FALSE, echo = FALSE}
X1 = AirBnB_prices["dist"]
X1 = sm.add_constant(X1)

Y = AirBnB_prices["realSum"]

model1 = sm.OLS(Y, X1).fit()
model1.summary()
```

```{r slr-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

2. You might remember that the `dir()` function can identify different objects within another object, and that in `statsmodels` objects, there are lot of useful, pre-calculated objects. Apply it to `model1`.

```{python setup1_5, exercise=FALSE, echo=FALSE, exercise.setup="setup1"}
X1 = AirBnB_prices["dist"]
X1 = sm.add_constant(X1)

Y = AirBnB_prices["realSum"]

model1 = sm.OLS(Y, X1).fit()
```

```{python dir, exercise = TRUE, message = FALSE, exercise.setup="setup1_5"}

```

```{python dir-solution, message = FALSE, warning = FALSE, echo = FALSE}
dir(model1)
```

```{r dir-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q3, echo=FALSE}
question("3. Which object do you think contains the predicted values for this model?",
         answer("`resid`", message = "`resid` gives you the residuals, which are calculated using the predicted values, but are not the same."),
         answer("`fittedvalues`", correct=TRUE, message = "Fitted values is another way of saying predicted values."),
         answer("`predict`", message = "Close! `predict` is actually a method that will predict a value using the line for a new observation."),
         answer("`pvalues`", message = "You might think `pvalues` stands for predicted values, but they're actually a totally different thing."), 
         allow_retry = TRUE, 
         random_answer_order = TRUE, 
         post_message = "Congratulations! You have found the first secret word: QUALITY.")
```

```{r q4, echo=FALSE}
question("4. Which object do you think contains the residuals for this model?",
         answer("`resid`", correct = TRUE),
         answer("`fittedvalues`", message = "We do need the predicted values to calculate the residuals, but we also need the observed values as well."),
         answer("`resid_pearson`", message = "Close! `resid_pearson` is actually a slightly different kind of residual, which you may learn more about in DATA213."),
         answer("`df_resid`", message = "You might think `df_resid` stands for DataFrame of residuals, but it is actually a different quantity called the degrees of freedom of the residuals."), 
         allow_retry = TRUE, 
         random_answer_order = TRUE)
```

There are many other useful things to unpack from a `statsmodels` object, but for now, let's focus on using `fittedvalues` and `resid` to create diagnostic plots.

## Diagnostic Plots

5. Edit the cell below to extract the predicted values and the residuals and turns them into a DataFrame (which we need to use for `plotnine`).

```{python setup2, exercise=FALSE, echo=FALSE, exercise.setup="setup1_5"}
X1 = AirBnB_prices["dist"]
X1 = sm.add_constant(X1)

Y = AirBnB_prices["realSum"]

model1 = sm.OLS(Y, X1).fit()

AirBnB_prices['logRealSum'] = np.log(AirBnB_prices['realSum'])
```

```{python dp_extract, exercise = TRUE, message = FALSE, exercise.setup="setup2"}
## Extracting the predicted values from the model and
## putting them in a DataFrame.
model1_df = model1.fittedvalues.to_frame(name = ___ )

## Adding a column of the residuals to the DataFrame.
model1_df[___] = model1.___
```

```{python dp_extract-solution, message = FALSE, warning = FALSE, echo = FALSE}
## Extracting the predicted values from the model and
## putting them in a DataFrame.
model1_df = model1.fittedvalues.to_frame(name = 'Fitted')

## Adding a column of the residuals to the DataFrame.
model1_df['Residuals'] = model1.resid
```

```{r dp_extract-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

6. Now, using `model1_df`, edit the cell to produce the diagnostic plots (a scatterplot of residuals against the predicted values, and a histogram of the residuals).

```{python setup2_5, exercise=FALSE, echo=FALSE, exercise.setup="setup2"}
model1_df = model1.fittedvalues.to_frame(name = 'Fitted')
model1_df['Residuals'] = model1.resid
```

```{python dp_produce, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup2_5"}
(p9.ggplot(model1_df, p9.aes(x = ___, y = ___)) +
 p9.geom_point() +
 p9.xlab("Predicted Price") +
 p9.ylab("Residuals"))
             
(p9.ggplot(model1_df, p9.aes(x = ___)) +
   p9.geom____(bins = 230) +
   p9.xlab("Residuals"))
```

```{python dp_produce-solution, message = FALSE, warning = FALSE, echo = FALSE}
(p9.ggplot(model1_df, p9.aes(x = 'Fitted', y = 'Residuals')) +
 p9.geom_point() +
 p9.xlab("Predicted Price") +
 p9.ylab("Residuals"))
             
(p9.ggplot(model1_df, p9.aes(x = 'Residuals')) +
   p9.geom_histogram(bins = 230) +
   p9.xlab("Residuals"))
```

```{r dp_produce-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q7, echo=FALSE}
question("7. Describe the distribution of the residuals.",
         answer("Symmetric"),
         answer("Right skewed", correct=TRUE, message="The linear regression assumption of symmetric residuals is violated."),
         answer("Left skewed", message = "The distribution is skewed, but look at the direction of the tail."), 
         allow_retry = TRUE, 
         random_answer_order = TRUE,
         post_message = "Congratulations! You have found the second secret word: SEPARATE.")
```

8. Sometimes when the response variable is particularly skewed, we can easily address failed assumptions by applying a mathematical transformation to the different variables. Money-related distributions are almost always skewed, but this one is particularly bad in part due to a few very extreme values (specifically, a 1,854,545 euro per night one bedroom home in Athens). 

To address the skewness, apply the log (base $e$) transformation to `realSum`. Save the transformed values as a new column in your data frame (name it `logRealSum`). Plot the distribution of the transformed variable and re-examine the histogram.

```{python log, exercise = TRUE, message = FALSE, exercise.setup="setup2_5"}
AirBnB_prices['logRealSum'] = np.log(___)

(p9.ggplot(AirBnB_prices, p9.aes(x = ___)) +
  p9.geom_histogram(bins = 24))
```

```{python log-solution, message = FALSE, warning = FALSE, echo = FALSE}
AirBnB_prices['logRealSum'] = np.log(AirBnB_prices['realSum'])

(p9.ggplot(AirBnB_prices, p9.aes(x = 'logRealSum')) +
  p9.geom_histogram(bins = 24))
```

```{r log-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

This distribution is far less skewed! The center and spread have also changed since we are working on a new scale, but is the shape that we are primarily concerned with. There are still a handful of extreme values, but nothing quite as bad as before.

9. Now, edit this cell to fit a new model predicting `logRealSum` from `dist`. Make a note of the $R^2$. Examine the diagnostic plots from the new model, which you should call `model2`.

```{python log_diag, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup2_5"}
Y2 = AirBnB_prices["logRealSum"]

model2 = sm.OLS(___, ___).fit()
model2.summary()

model2_df = model2.fittedvalues.to_frame(name = 'Fitted')
model2_df['Residuals'] = model2.resid

(p9.ggplot(model2_df, p9.aes(x = 'Fitted', y = 'Residuals')) +
 p9.geom_point() +
 p9.xlab("log Predicted Price") +
 p9.ylab("Residuals"))
 
(p9.ggplot(model2_df, p9.aes(x = 'Residuals')) +
   p9.geom_histogram(bins = 24) +
   p9.xlab("Residuals"))
```

```{python log_diag-solution, message = FALSE, warning = FALSE, echo = FALSE}
Y2 = AirBnB_prices["logRealSum"]

model2 = sm.OLS(Y2, X1).fit()
model2.summary()

model2_df = model2.fittedvalues.to_frame(name = 'Fitted')
model2_df['Residuals'] = model2.resid

(p9.ggplot(model2_df, p9.aes(x = 'Fitted', y = 'Residuals')) +
 p9.geom_point() +
 p9.xlab("log Predicted Price") +
 p9.ylab("Residuals"))
 
(p9.ggplot(model2_df, p9.aes(x = 'Residuals')) +
   p9.geom_histogram(bins = 24) +
   p9.xlab("Residuals"))
```

```{r log_diag-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q10, echo=FALSE}
question("10. True or False: A linear model is more appropriate for these two variables.",
         answer("True", correct=TRUE),
         answer("False"), 
         allow_retry = TRUE)
```

The $R^2$ is still very small, and the residual plot still looks somewhat patterned, although slightly less so. The biggest change is in the distribution of the residuals, which is still not entirely symmetric but is much closer to meeting the assumption.

Transforming the response variable is one common technique for addressing violated assumptions in linear regression. The log transform is probably the most frequently used, but you can also try logs with different bases, the square root or other exponents, the inverse, or double log, among others!

## Significance Testing with Simple Linear Regression

11. The $R^2$ for both models is fairly low, so we already have some idea of whether `dist` is a helpful predictor. Let's practice formally finding significant features in the model.

```{r q11, echo=FALSE}
question("Given $H_0: \\beta_{dist} = 0$, what is an appropriate alternative hypothesis?",
         answer("$H_A: \\beta_{dist} > 0$"),
         answer("$H_A: \\beta_{dist} \\neq 0$", correct=TRUE),
         answer("$H_A: \\beta_{dist} < 0$"), 
         allow_retry = TRUE, 
         post_message = "Congratulations! You have found the third secret word: BACKGROUND.")
```

12. Print the summary output from `model2`.

```{python setup3, exercise=FALSE, echo=FALSE, exercise.setup="setup2_5"}
Y2 = AirBnB_prices["logRealSum"]

model2 = sm.OLS(Y2, X1).fit()

model2_df = model2.fittedvalues.to_frame(name = 'Fitted')
model2_df['Residuals'] = model2.resid
```

```{python m2, exercise=TRUE, message = FALSE, exercise.setup="setup3"}
model2.___
```

```{python m2-solution, message = FALSE, warning = FALSE, echo = FALSE}
model2.summary()
```

```{r m2-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q13, echo=FALSE}
question("13. Would you reject or fail to reject a formal hypothesis test investigating whether or not the coefficient is zero?",
         answer("Reject"),
         answer("Fail to reject", correct=TRUE, message = "A conclusion of this test in context would be that there is not enought evidence to say that distance is a significant predictor of log AirBnB price per night in ten major European cities."), 
         allow_retry = TRUE)
```

## Multiple Linear Regression

14. `dist` may be significant, but as we can see from the adjusted $R$-squared, it doesn't predict `logRealSum` very well. Let's try adding more variables! Run this cell to take a look at `AirBnB_prices` and use it to answer the following question--again, check out the [Kaggle documentation](https://www.kaggle.com/datasets/thedevastator/airbnb-prices-in-european-cities) if you need.

```{python col, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup3"}
AirBnB_prices.columns
```

```{r q14, echo=FALSE}
question("Some variables are likely collinear--can you identify any pairs that might give you trouble later on? Select ALL that apply.",
         answer("`attr_index` and `attr_index_norm`", correct=TRUE),
         answer("`rest_index` and `rest_index_norm`", correct=TRUE),
         answer("`person_capacity` and `bedrooms`", correct=TRUE),
       answer("`cleanliness_rating` and `guest_satisfaction_overall`", correct=TRUE),
         answer("`dist` and `metro_dist`"),
         answer("`lng` and `lat`"),         
         allow_retry = TRUE,
         random_answer_order = TRUE)
```

15. To confirm assumptions, edit the code so that it will calculate the correlation matrix for the numeric variables (we can ignore indicators for Lab 3). For easier reading, I've rounded to three decimals. 

```{python cormat, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup3"}
round(AirBnB_prices[[ ___ ]].corr(), 3)
```

```{python cormat-solution, message = FALSE, warning = FALSE, echo = FALSE}
round(AirBnB_prices[['logRealSum', 'person_capacity', 'cleanliness_rating', 'guest_satisfaction_overall', 'bedrooms', 'dist', 'metro_dist', 'attr_index', 'attr_index_norm', 'rest_index', 'rest_index_norm', 'lng', 'lat']].corr(), 3)
```

```{r cormat-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

Take a closer look at the correlation matrix--are the results consistent with your answer to question 14? You may have located even more problematic pairings (anything over 0.8 is concerning), but based on the documentation, you should have found that `attr_index` and `attr_index_norm`, as well as `rest_index` and `rest_index_norm`, are just normalized versions of each other. To avoid collinearity issues, we should only include one version. 

We are not covering multicollinearity in this lab (instead, see Lab 3.5), but it is important to know that you can address it by considering the context of your variables before you even select the model! So, be careful with your data dictionaries and pay attention to the variables that you are working with. 

16. Now let's try multiple linear regression. Run the cell below to fit a model predicting `realSum` with the numeric variables--be careful to use only one set of the `attr` and `rest` variables.

```{python setup4, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup3"}
X3 = AirBnB_prices[['person_capacity', 'cleanliness_rating', 'guest_satisfaction_overall', 'bedrooms', 'dist', 'metro_dist', 'attr_index_norm', 'rest_index_norm', 'lng', 'lat']]
X3 = sm.add_constant(X3)

model3 = sm.OLS(Y, X3).fit()
model3.summary()
```

```{r q16, echo=FALSE}
question("What is the adjusted $R^2$?",
         answer("0.141", message = "That is the $R^2$! Don't forget to look for the adjusted $R^2$."),
         answer("0.125", correct=TRUE),
         answer("1.34e-13"),
         answer("1.76e+03"),
         allow_retry = TRUE,
         random_answer_order = TRUE)
```

17. Run this cell to look at the diagnostic plots of this model.

```{python diagplots, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup4"}
model3_df = model3.fittedvalues.to_frame(name = 'Fitted')
model3_df['Residuals'] = model3.resid

(p9.ggplot(model3_df, p9.aes(x = 'Fitted', y = 'Residuals')) +
 p9.geom_point() +
 p9.xlab("Predicted Price") +
 p9.ylab("Residuals"))

(p9.ggplot(model3_df, p9.aes(x = 'Residuals')) +
   p9.geom_histogram(bins = 24) +
   p9.xlab("Residuals"))
```

```{r q18, echo=FALSE}
question("18. True or False: The linear model is appropriate for this dataset.",
         answer("True"),
         answer("False", correct=TRUE),
         allow_retry = TRUE)
```

19. Don't forget that that the transformed response had much better diagnostic plots for the simple linear regression! Run the cell below to predict `logRealSum`.

```{python setup5, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup4"}
model4 = sm.OLS(Y2, X3).fit()
model4.summary()
```

```{r q20, echo=FALSE}
question("20. What is the adjusted $R^2$?",
         answer("0.453"),
         answer("0.443", correct=TRUE),
         answer("0.510"),
         answer("0.629"),
         allow_retry = TRUE,
         random_answer_order = TRUE)
```

21. Redo the diagnostic plots (use the code above as a reference if you need!).

```{python diagnostic_plots_log, exercise=TRUE, message = FALSE, exercise.setup="setup5"}

```

```{python diagnostic_plots_log-solution, message = FALSE, warning = FALSE, echo = FALSE}
model4_df = model4.fittedvalues.to_frame(name = 'Fitted')
model4_df['Residuals'] = model4.resid

(p9.ggplot(model4_df, p9.aes(x = 'Fitted', y = 'Residuals')) +
 p9.geom_point() +
 p9.xlab("Predicted Price") +
 p9.ylab("Residuals"))

(p9.ggplot(model4_df, p9.aes(x = 'Residuals')) +
   p9.geom_histogram(bins = 24) +
   p9.xlab("Residuals"))
```

```{r diagnostic_plots_log-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q22, echo=FALSE}
question("22. True or False: A linear model is more appropriate for this data set.",
         answer("True", correct=TRUE),
         answer("False"),
         allow_retry = TRUE,
         post_message = "Congratulations! You have found the fourth secret word: SAMPLE.")
```

Note that in the summary there was a warning about a large condition number. This warning has to do with multicollinearity, and tells us that we need to eliminate some of the variables. For more on multicollinearity, check out Lab 3.5.

## Significance Testing with Multiple Linear Regression

Use the summary of the multiple linear regression model to answer the following questions. 

```{python mlrsummary, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup5"}
model4.summary()
```

```{r q23, echo=FALSE}
question("23. Which of the variables are significant at the 5% level? Select ALL that apply.",
         answer("`const`", correct = TRUE),
         answer("`person_capacity`", correct = TRUE),
         answer("`cleanliness_rating`"),
         answer("`guest_satisfaction_overall`"),
         answer("`bedrooms`", correct = TRUE),
         answer("`dist`", correct = TRUE),
         answer("`metro_dist`"),
         answer("`attr_index_norm`", correct = TRUE),
         answer("`rest_index_norm `"),
         answer("`lng`", correct = TRUE),
         answer("`lat`", correct = TRUE),
         allow_retry = TRUE,
         post_message = "Congratulations! You have found the fifth and final secret word: SHAPE.")
```

```{r q24, echo=FALSE}
question("24. For every one bedroom increase, log price is expected to  ",
         answer("increase by 0.4324.", correct = TRUE),
         answer("decrease by 0.4324.", message = "Be careful of the sign!"),
         answer("increase by 0.058.", message = "That's the standard error!"),
         answer("decrease by 0.058."),
         allow_retry = TRUE,
         random_answer_order = TRUE
         )
```

