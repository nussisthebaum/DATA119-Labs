---
title: "DATA119 - Lab 2"
output: 
   learnr::tutorial:
      css: css/custom-styles.css
runtime: shiny_prerendered
---

```{r setup, include = FALSE}
library(learnr)
library(gradethis)
library(reticulate)

knitr::opts_chunk$set(error = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)

# Set the path to the existing Python environment
#reticulate::use_python("/opt/python/3.9.21/bin/python", required = TRUE)

# Optional: Install necessary Python packages if not already installed
#system("/opt/python/3.9.21/bin/python -m pip install numpy pandas plotnine")

custom_checker <- function(label, user_code, solution_code, envir_result, evaluate_result, last_value, stage, ...) {
  if (stage == "code_check") {
      if (user_code == solution_code){
          return(list(message = random_praise(), correct = TRUE))
      }
    return(list(message = random_encouragement(), correct = FALSE))
  }
}

gradethis_setup()
tutorial_options(exercise.completion = FALSE, exercise.checker = custom_checker)
```

```{r header, echo=FALSE}
library(htmltools)

tags$div(
  class = "topContainer",
  tags$div(
    class = "logoAndTitle",
    tags$img(
      src = "./images/dsi_logo.png",
      alt = "DSI Logo",
      class = "topLogo"
    ),
    tags$h1("Categorical Variables and Multiple Linear Regression", class = "pageTitle")
  )
)
```

## Goals

* To continue practicing making data visualizations.
* To familiarize yourself with Pearson's correlation coefficient $R$.
* To practice writing functions.
* To practice calculating linear regression estimates.

## Setup

For this lab we will be using the `plotnine`, `pandas`, and `numpy` modules in Python, as well as three new datasets: `eagles.csv`, `gasprice.csv`, and `mtcars.csv`. Finally, you will be using the cleaned version of the Epicurious dataset from Lab 1. Run the cell below to setup the environment.

```{python setup1, exercise=TRUE, exercise.eval = FALSE}
import numpy as np
import pandas as pd
import plotnine as p9

mtcars = pd.read_csv("http://raw.githubusercontent.com/nussisthebaum/DATA119-Labs/refs/heads/main/data/mtcars.csv")
gasprice = pd.read_csv("http://raw.githubusercontent.com/nussisthebaum/DATA119-Labs/refs/heads/main/data/gasprice.csv")
eagles = pd.read_csv("http://raw.githubusercontent.com/nussisthebaum/DATA119-Labs/refs/heads/main/data/eagles.csv")

epicurious_nd = pd.read_csv("http://raw.githubusercontent.com/nussisthebaum/DATA119-Labs/refs/heads/main/data/epi_mini_clean.csv")
```


## Describing Relationships with `mtcars`

1. The `mtcars` dataset includes data on various aspects of 32 different cars, extracted from the 1974 US issue of *Motor Trend* magazine (check out the [`mtcars` documentation here](https://vincentarelbundock.github.io/Rdatasets/doc/datasets/mtcars.html)). Using `plotnine`, create a scatterplot displaying the relationship between `weight` and `displacement` in `mtcars`. Remember that you should be supplying an `x` and a `y` aesthetic, and that `x` is traditionally the explanatory variable and `y` is traditionally the response.

```{python scatter, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
(p9.ggplot(mtcars, p9.aes(x = ___, y = ___)) +
  p9.geom_point())
```

```{python scatter-solution, message = FALSE, warning = FALSE, echo = FALSE}
(p9.ggplot(mtcars, p9.aes(x = 'wt', y = 'disp')) +
  p9.geom_point())
```

```{r scatter-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

2. Note that `plotnine` has a tool for us to easily add the line of best fit to a plot--it is the function `geom_smooth()`. We need to specify that geom_smooth should be using linear regression, which we can do with the argument `method = "lm"`. Add this argument to the previous graph

```{python lm, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
(p9.ggplot(mtcars, p9.aes(x = 'wt', y = 'disp')) +
  p9.geom_point())
```

```{python lm-solution, message = FALSE, warning = FALSE, echo = FALSE}
(p9.ggplot(mtcars, p9.aes(x = 'wt', y = 'disp')) +
  p9.geom_point() + 
  p9.geom_smooth(method = "lm"))
```

```{r lm-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

Note that this added line includes something called standard error bars, which we will not be covering in this class. If you would like to turn them off to make things visually cleaner, you can use the argument `se = False`, like so:

```{python lm_no_se, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
(p9.ggplot(mtcars, p9.aes(x = 'wt', y = 'disp')) +
  p9.geom_point() + 
  p9.geom_smooth(method = "lm", se = False))
```

```{r q3, echo=FALSE}
question("3. Is a linear relationship appropriate for describing the relationship between weight and displacement? If not, what type of relationship might be more appropriate?",
         answer("Linear", message = "There is not an even distribution of points above and below the line, look closely at the middle of the graph."),
         answer("Quadratic", correct = TRUE),
         answer("Exponential"), 
         allow_retry = TRUE, 
         random_answer_order = TRUE, 
         post_message = "Congratulations! You have found the first secret word: SUPPRESS.")
```

## Describing Relationships with `gasprice`

4. The `gasprice` dataset includes the average price of gas in the US for every week from February 1990 to June 2003 (check out the [`gasprice` documentation here](https://vincentarelbundock.github.io/Rdatasets/doc/quantreg/gasprice.html)). Using `plotnine`, create a scatterplot displaying the relationship between `time` and `value` (the price of gas). Add the line of best fit to help you describe the plot.

```{python gas, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
(p9.ggplot(___, ___) +
  p9.geom____())
```

```{python gas-solution, message = FALSE, warning = FALSE, echo = FALSE}
(p9.ggplot(gasprice, p9.aes(x = 'time', y = 'value')) +
  p9.geom_point() + 
  p9.geom_smooth(method = "lm", se = False))
```

```{r gas-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```


```{r q5, echo=FALSE}
question("5. Is a linear relationship appropriate for describing the relationship between time and price? If not, what type of relationship might be more appropriate?",
         answer("True", message = "This is an example of a time series, which should not be analyzed using linear regression."),
         answer("False", correct = TRUE, message = "This is an example of a time series, which should not be analyzed using linear regression"), 
         allow_retry = TRUE, 
         post_message = "Congratulations! You have found the first second word: COMMISSION.")
```

## Describing Relationships with `eagles`

The `eagles` dataset contains information on the number of mating pairs of bald eagles in the United States. In 1967, bald eagles officially became an endangered species, and in 1972, the use of DDT (a pesticide causing damage to the shells of the eagle eggs) was banned--[scientists have been tracking the population since](https://courses.lumenlearning.com/wmopen-concepts-statistics/chapter/exponential-relationships-1-of-6/).

6. Create a scatterplot displaying the relationship between time (`Year`) and the number of mating pairs (`Pairs`). Add the line of best fit to help you describe the plot.

```{python eagles, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
(p9.ggplot(___, ___) + 
  ___ ...)
```

```{python eagles-solution, message = FALSE, warning = FALSE, echo = FALSE}
(p9.ggplot(eagles, p9.aes(x = 'Year', y = 'Pairs')) +
  p9.geom_point() + 
  p9.geom_smooth(method = "lm", se = False))
```

```{r eagles-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q7, echo=FALSE}
question("7. Is a linear relationship appropriate for describing the relationship between `Year` and `Pairs`? If not, what type of relationship might be more appropriate?",
         answer("Linear", message = "Look closely at the graph, all of the points below the line are clustered together, and all of the points above the line are clustered together."),
         answer("Quadratic"),
         answer("Exponential", correct = TRUE, message = "Exponential curves are often used to model growth and decay.", 
         allow_retry = TRUE, 
         random_answer_order = TRUE, 
         post_message = "Congratulations! You have found the third secret word: MOON."))
```

Now to confirm the value of the correlation between `Year` and `Pairs` you can use `.corr()`. Run this cell to get the value of $R$.

```{python corr, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup1"}
Year = eagles['Year']
Pairs = eagles['Pairs']

Year.corr(Pairs)
```

```{r q8, echo=FALSE}
question("8. Would you describe this as weak, moderate, or strong based on $R$?",
         answer("Weak"),
         answer("Moderate"),
         answer("Strong", correct=TRUE, message = "This is a great example of why you need to look at the correlation and the scatterplot! Even though this relationship is not linear, it appears to be a strong linear relationship based on $R$."), 
         allow_retry = TRUE)
         #post_message = "Congratulations! You have found the first secret word: SUPPRESS.")
```

## Describing Relationships with `epicurious_nd`

9. For the remaining exercises, we will use the `epicurious_nd` dataset, treating `fat` as an explanatory variable and `calories` as the response variable. Create a scatterplot displaying the relationship between `fat` and `calories.` Be sure to put the variables on the appropriate axes.

```{python fat, exercise = TRUE, message = FALSE, exercise.setup="setup1"}

```

```{python fat-solution, message = FALSE, warning = FALSE, echo = FALSE}
(p9.ggplot(epicurious_nd, p9.aes(x = 'fat', y = 'calories')) +
  p9.geom_point() + 
  p9.geom_smooth(method = "lm", se = False))
```

```{r fat-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q10, echo=FALSE}
question("10. Is a linear relationship appropriate for describing the relationship between fat and calories? If not, what type of relationship might be more appropriate?",
         answer("Linear", correct=TRUE, message = "There are a few unusual values, but this is a good example of a linear relationship!"),
         answer("Quadratic"),
         answer("Exponential"))
```

11. Now, define lists `fat` and `cal`, and calculate the correlation between fat and calories.

```{python fatcor, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
fat = 
cal = 
```

```{python fatcor-solution, message = FALSE, warning = FALSE, echo = FALSE}
fat = epicurious_nd['fat']
cal = epicurious_nd['calories']

fat.corr(cal)
```

```{r fatcor-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

12. There are 28.346 grams of fat in an ounce. Create a new list called `fat_oz` containing the amount fat in the recipe in ounces. Now, calculate the correlation between `fat_oz` and `calories`. 

```{python fatoz, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
fat_oz = 
cal = epicurious_nd['calories']

```

```{python fatoz-solution, message = FALSE, warning = FALSE, echo = FALSE}
fat_oz = epicurious_nd['fat']/28.346
cal = epicurious_nd['calories']

fat_oz.corr(cal)
```

```{r fatoz-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

You should have found that the correlations that you just calculated are identical. This is because correlation is **unitless**--it does not depend on the units of either of the variables. Similarly, the order doesn't matter. Run the cell below to confirm.

```{python switch, exercise=TRUE, exercise.eval = FALSE, message = FALSE, exercise.setup="setup1"}
cal.corr(fat)
```


## Writing Functions

### Slope

Occasionally we want to perform calculations and repeat them over and over again--this is an excellent place to write a function.     

13. Using the equation $b_1 = r \times \frac{s_y}{s_x}$, fill out the function below called `calc_slope` that will calculate the estimate of the slope, $b_1$, given the correlation between two variables, the standard deviation of the explanatory variable, and the standard deviation of the response. 

```{python slope, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
def calc_slope(r_xy, s_x, s_y):
    b1 = 
    return(b1)

```

```{python slope-solution, message = FALSE, warning = FALSE, echo = FALSE}
def calc_slope(r_xy, s_x, s_y):
    b1 = r_xy*s_y/s_x 
    return(b1)
```

```{r slope-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

14.  Now, apply your function to the `epicurious_nd` data. You will need the correlation and standard deviations--save the correlation as `r_fatcal` and the standard deviations as `s_fat` and `s_cal`.

```{python setup2, exercise=FALSE, echo=FALSE, exercise.setup="setup1"}
fat = epicurious_nd['fat']
cal = epicurious_nd['calories']

def calc_slope(r_xy, s_x, s_y):
    b1 = r_xy*s_y/s_x 
    return(b1)
```


```{python slopec, exercise = TRUE, message = FALSE, exercise.setup="setup2"}
r_fatcal = 
s_fat = 
s_cal = 


```

```{python slopec-solution, message = FALSE, warning = FALSE, echo = FALSE}
r_fatcal = fat.corr(cal)
s_fat = epicurious_nd['fat'].std()
s_cal = epicurious_nd['calories'].std()

calc_slope(r_fatcal, s_fat, s_cal)
```

```{r slopec-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q15, echo=FALSE}
question("15. Fill in the blank: for every additional ___, we should expect 12.73 additional ___",
         answer("gram of fat; calories", correct=TRUE),
         answer("calorie; grams of fat", message = "Consider what the independent and dependent variables are."), 
         allow_retry = TRUE, 
         post_message = "Congratulations! You have found the fourth secret word: ASTONISHING.")
```

### Intercept 

16. We also need to frequently calculate the intercept. Using the equation $b_0 = \bar{y} - b_1\bar{x}$ write a function called `calc_int` that will calculate the estimate of the intercept, $b_0$, given the correlation between two variables, the standard deviations of both the explanatory and response variables, and the means of both the explanatory and response variables. Your new function should use the `calc_slope` function you just wrote.

```{python int, exercise = TRUE, message = FALSE, exercise.setup="setup2"}
def calc_int(r_xy, s_x, s_y, m_x, m_y):
    b1 = 
    b0 = 
    return(b0)

```

```{python int-solution, message = FALSE, warning = FALSE, echo = FALSE}
def calc_int(r_xy, s_x, s_y, m_x, m_y):
    b1 = calc_slope(r_xy, s_x, s_y)
    b0 = m_y - b1*m_x
    return(b0)
```

```{r int-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

17. Now apply your function to the `epicurious_nd` data. Save the means as `m_fat` and `m_cal`. 

```{python setup3, exercise=FALSE, echo=FALSE, exercise.setup="setup2"}
def calc_int(r_xy, s_x, s_y, m_x, m_y):
    b1 = calc_slope(r_xy, s_x, s_y)
    b0 = m_y - b1*m_x
    return(b0)
```

```{python intc, exercise = TRUE, message = FALSE, exercise.setup="setup3"}
m_fat = 
m_cal = 


```

```{python intc-solution, message = FALSE, warning = FALSE, echo = FALSE}
m_fat = epicurious_nd['fat'].mean()
m_cal = epicurious_nd['calories'].mean()

calc_int(r_fatcal, s_fat, s_cal, m_fat, m_cal)
```

```{r intc-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

```{r q18, echo=FALSE}
question("18. Fill in the blank: if the recipe has 0 ___ then we should expect it to have 132.86 ___",
         answer("calories; grams of fat"),
         answer("grams of fat; calories", correct=TRUE), , 
         allow_retry = TRUE, 
         post_message = "Congratulations! You have found the fifth secret word: ARCHITECT.")

```

### Prediction

19. One of the linear regression tasks we are usually interested in is prediction. Write a function called `calc_pred` that can predict values of $y$ from $x$ given a column of data for the explanatory variable and a column of data for the response. Your new function should use both the `calc_slope` and `calc_int` functions. 

```{python pred, exercise = TRUE, message = FALSE, exercise.setup="setup3"}
def calc_pred(x, y):
    r_xy = 
    
    m_x = 
    m_y = 
    
    sd_x = 
    sd_y = 
    
    b1 = 
    b0 = 
    
    preds = 
    
    return(preds)
```

```{python pred-solution, message = FALSE, warning = FALSE, echo = FALSE}
def calc_pred(x, y):
    r_xy = x.corr(y)
    
    m_x = x.mean()
    m_y = y.mean()
    
    sd_x = x.std()
    sd_y = y.std()
    
    b1 = calc_slope(r_xy, sd_x, sd_y)
    b0 = calc_int(r_xy, sd_x, sd_y, m_x, m_y)
    
    preds = b0 + b1*x
    
    return(preds)
```

```{r pred-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

20. Now, apply the function to the `epicurious_nd` dataset and save a list of calorie predictions as `pred_cal`. 

```{python setup4, exercise=FALSE, echo=FALSE, exercise.setup="setup3"}
def calc_pred(x, y):
    r_xy = x.corr(y)
    
    m_x = x.mean()
    m_y = y.mean()
    
    sd_x = x.std()
    sd_y = y.std()
    
    b1 = calc_slope(r_xy, sd_x, sd_y)
    b0 = calc_int(r_xy, sd_x, sd_y, m_x, m_y)
    
    preds = b0 + b1*x
    
    return(preds)
```


```{python predc, exercise = TRUE, message = FALSE, exercise.setup="setup4"}
fat = epicurious_nd['fat']
cal = epicurious_nd['calories']

pred_cal = 
```

```{python predc-solution, message = FALSE, warning = FALSE, echo = FALSE}
fat = epicurious_nd['fat']
cal = epicurious_nd['calories']

pred_cal = calc_pred(fat, cal)
```

```{r predc-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

### Residuals

21. We will definitely need to repeatedly calculate the residuals. Write a function called `calc_res` that can calculate the residuals for the entire dataset, given a column of data for the explanatory variable, and a column of data for the response. Your new function should use at least one of the functions you have already written.

```{python res, exercise = TRUE, message = FALSE, exercise.setup="setup4"}
def calc_res(x, y):  
    preds = 
    resids = 
    return(resids)
```

```{python res-solution, message = FALSE, warning = FALSE, echo = FALSE}
def calc_res(x, y):  
    preds = calc_pred(x, y)
    resids = y - preds
    return(resids)
```

```{r res-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

22. Now, apply the function to the `epicurious_nd` dataset and save a list of residuals as `pred_res`. 

```{python setup5, exercise=FALSE, echo=FALSE, exercise.setup="setup4"}
def calc_res(x, y):  
    preds = calc_pred(x, y)
    resids = y - preds
    return(resids)
```

```{python resc, exercise = TRUE, message = FALSE, exercise.setup="setup5"}
pred_res
```

```{python resc-solution, message = FALSE, warning = FALSE, echo = FALSE}
pred_res = calc_res(fat, cal)
```

```{r resc-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

### Loss Function and Optimization

The loss function for simple linear regression is the sum of the squared residuals (also known as the sum of squared errors, or SSE). Below is an example of what an SSE function can look like.

```{python sse, exercise=FALSE, exercise.setup="setup5"}
def calc_SSE(x, y):  
    resids = calc_res(x, y)
    resids2 = resids**2

    return(resids2.sum())
```

We are lucky to have equations (more formally known as closed form estimators) to estimate the slope and the intercept. However, this is not always the case! Recall that the estimates are derived from minimizing the sum of the squared residuals--when we don't have closed form estimators (which happens frequently in more complicated models), we will have to use computers to minimize our loss function. Then, the estimates we are looking for are the values where the loss function is minimized.

Minimizing a loss function is a special case of optimization. To optimize, we will need the `scipy`. From `scipy.optimize`, we specifically need `minimize`.

```{python scipy, exercise=FALSE, exercise.setup="sse"}
import scipy
from scipy.optimize import minimize
```

The `minimize` method accepts many different arguments, but here's what we are specifically concerned about:

* `fun`, the objective function to be minimized.
* `x0`, an initial guess for the values we are looking for. 

You can read more about the other arguments, which may come into play with more complicated models, in the [scipy.optimize.minimize documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html). You will have to specify a method, for most problems, `method="BFGS"` will work.

In this case, the objective function is the loss function we happen to be using--in the case of linear regression, the SSE function. We can't just use the function as is, it has to have a special format. 

* The first argument needs contain the values that you are looking for, in this case, the slope and intercept. Other things you need can go next. 
* The function must return the loss function, in this case, the SSE. 

23. Adapt the example `calc_SSE` function so that it follows these requirements. 

```{python sseo, exercise = TRUE, message = FALSE, exercise.setup="scipy"}
def calc_SSE(parms, x, y):  
    preds = 
    resids = 
    resids2 = 
    return(resids2.sum())
```

```{python sseo-solution, message = FALSE, warning = FALSE, echo = FALSE}
def calc_SSE(parms, x, y):  
    preds = parms[0] + parms[1]*x
    resids = y - preds
    resids2 = resids**2
    return(resids2.sum())
```

```{r sseo-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

We can give any real values for the initial guesses, but we have to give at least two, one for the intercept and one for the slope. For now, we can just use zero for both. With both of these pieces, we can run `minimize()`

```{python minset, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="scipy"}
def calc_SSE(parms, x, y):  
    preds = parms[0] + parms[1]*x
    resids = y - preds
    resids2 = resids**2
    return(resids2.sum())

fat = epicurious_nd['fat']
cal = epicurious_nd['calories']
```


```{python min, exercise = TRUE, message = FALSE, exercise.setup="minset"}
init = [0]*2
minimize(calc_SSE, x0 = init, args = (fat, cal), method = "BFGS").x
```

You should find that these estimates are very close to the numbers that we calculated using the equations above.

## Using `statsmodels`

In addition to writing your own functions, either to directly estimate parameters or to minimize the loss function, there are a lot of pre-packaged methods in Python to use for linear regression. You may be familiar with `scikit-learn` already, but I want to introduce another module, `statsmodels`. Both modules work similarly, but `statsmodels` has some functionality that will help us out in future weeks. 

```{python stats, exercise = TRUE, message = FALSE, exercise.setup="scipy"}
import statsmodels.api as sm
```

To fit a linear regression model with `statsmodels`, we first have to separately save our $X$ and $Y$ variables. Technically, we've already done that with `fat` and `calories`, but review the code below--it's good practice to get into this type of syntax. I've used `X1` since this is our first (and only) variable. 

```{python stats1, exercise = TRUE, message = FALSE, exercise.setup="stats"}
X1 = epicurious_nd["fat"]
Y = epicurious_nd["calories"]
```

Note that we've only saved one explanatory variable here, but you could save more than one to use with multiple linear regression. 

By saving `"fat"` inside `X1`, we're letting Python know that we want a slope for fat, but we also need to specify that we would like an intercept as well. We do that by using the following code:

```{python stats2, exercise = TRUE, message = FALSE, exercise.setup="stats1"}
X1 = sm.add_constant(X1)
```

Now, we can actually fit the model. We do this with the following line of code (again, I've saved an object called `model1` since this is the first model we are fitting. Also, the only model for this lab, but it's good practice to get into the habit of giving different models different names so you don't write over anything and lose track of what you were doing. 

```{python stats3, exercise = TRUE, message = FALSE, exercise.setup="stats2"}
model1 = sm.OLS(Y, X1).fit()
```

`statsmodels` knows that people using linear regression are generally going to be performing similar calculations, so there is a lot packed into `model1`. 

24. Apply the `dir()` function to `model1`, and see if you can identify any of the parts packed into the object. 

```{python dir, exercise = TRUE, message = FALSE, exercise.setup="stats3"}

```

```{python dir-solution, message = FALSE, warning = FALSE, echo = FALSE}
dir(model1)
```

```{r dir-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

25. As I mentioned, there is a lot! For now, we just want to use the `.summary()` method. Investigate the output below to see if you can locate where in the table the slope and intercept are located. Are they the same as the values calculated from the other two methods we've seen?

```{python summary, exercise = TRUE, message = FALSE, exercise.setup="stats3"}
model1.summary()
```
